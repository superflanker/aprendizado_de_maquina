{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766799f3",
   "metadata": {
    "id": "766799f3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Objetivo\n",
    "Com este código, você irá:\n",
    "- automatizar o processo de otimizar $w$ e $b$ usando o Método do Gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa948848",
   "metadata": {
    "id": "aa948848",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Método do Gradiente para Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053334d",
   "metadata": {
    "id": "1053334d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ferramentas\n",
    "\n",
    "Nesse código, você usará:\n",
    "- NumPy, uma biblioteca popular para cálculos matriciais, etc\n",
    "- Matplotlib, uma biblioteca popular para gerar figuras a partir de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de66c47",
   "metadata": {
    "id": "4de66c47",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b3e2a",
   "metadata": {
    "id": "a62b3e2a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Definição do Problema\n",
    "\n",
    "\n",
    "Usaremos as mesmas amostras de dados que já utilizamos em códigos anteriores:\n",
    "\n",
    "| Corrente (A)     | Tensão (V) |\n",
    "| -------------------| ------------------------ |\n",
    "| 0.2               | 21                      |\n",
    "| 4.5               | 430                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b802b",
   "metadata": {
    "id": "5a3b802b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Carregando nosso conjunto de dados:\n",
    "x_trein = np.array([0.2, 4.5])       # característica\n",
    "y_trein = np.array([21.0, 430.0])    # valor alvo\n",
    "\n",
    "\n",
    "# regredindo no método chunchex o problema original - iremos utilizar somente o intercepto - dá mais graça às coisas\n",
    "w = (y_trein[1]  - y_trein[0]) / (x_trein[1] - x_trein[0])\n",
    "b = y_trein[0] - w * x_trein[0]\n",
    "\n",
    "# calculando os valores para um resistor de 50 ohms\n",
    "w = 50\n",
    "w_truth = w\n",
    "b_truth = b\n",
    "# gerando uma quantidade suficiente de pontos\n",
    "n_points = 1000\n",
    "x_min = 1\n",
    "x_max = 4\n",
    "sigma = 1\n",
    "mu = 0.0\n",
    "delta = np.sqrt(12 * sigma ** 2) / 2\n",
    "x_trein = np.linspace(x_min, x_max, n_points)\n",
    "\n",
    "# instanciando um gerador aleatório - agradeço à numpy por isto\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "# gerando as sequências\n",
    "y_trein = w * x_trein + b + rng.uniform(-delta, delta, n_points)\n",
    "# é só descomentar a próxima linha para mudar a bagunça que faremos\n",
    "# y_trein = w * x_trein + b + rng.normal(mu, sigma, n_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7456f",
   "metadata": {
    "id": "17b7456f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculando o Custo\n",
    "\n",
    "Essa parte foi desenvolvida no nosso último código. Vamos precisar aqui novamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a48aea",
   "metadata": {
    "id": "f2a48aea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calcula_custo(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula a função custo no âmbito da regressão linear.\n",
    "    Argumentos da função:\n",
    "      x (ndarray (m,)): Conjunto de dados com m amostras\n",
    "      y (ndarray (m,)): Valores alvo de saída\n",
    "      w,b (escalar)   : Parâmetros do modelo\n",
    "    Retorna\n",
    "      custo_total (float): O custo custo de se usar w,b como parâmetros na regressão linear\n",
    "               para ajustar os dados\n",
    "    \"\"\"\n",
    "    # número de amostras de treinamento\n",
    "    m = x.shape[0]\n",
    "\n",
    "    soma_custo = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        custo = (f_wb - y[i]) ** 2\n",
    "        soma_custo = soma_custo + custo\n",
    "    custo_total = (1 / (2 * m)) * soma_custo\n",
    "\n",
    "    return custo_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b53d2",
   "metadata": {
    "id": "573b53d2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Método do Gradiente\n",
    "Por enquanto, você já desenvolveu um modelo linear que estima $f_{w,b}(x^{(i)})$:\n",
    "\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "Na regressão linear, você utiliza dados de treinamento para ajustar os parâmetros $w$,$b$ minimizando o erro entre suas previsões $f_{w,b}(x^{(i)})$ e os dados verdadeiros $y^{(i)}$. A soma dos erros quadráticos para todas as amostras $(x^{(i)},y^{(i)})$ é chamada de custo, e é denotada por $J(w,b)$:\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae986d82",
   "metadata": {
    "id": "ae986d82",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "O Método do Gradiente então consiste em:\n",
    "\n",
    "$$\\begin{align*} \\text{repetir}&\\text{ até convergir:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline\n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "onde os parâmetros $w$, $b$ devem ser atualizados simultaneamente.  \n",
    "\n",
    "O gradiente é definido como:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Aqui, 'simultaneamente' significa que você deve calcular as derivadas parciais para todos os parâmetros antes de atualizar qualquer um desses parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed14caf",
   "metadata": {
    "id": "6ed14caf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Implementando o Método do Gradiente na Prática.\n",
    "Você irá implementar o método do gradiente para uma única variável (característica). Para fazer isso, você precisará de três funções:\n",
    "- `calcula_gradiente` implementando as equações (4) e (5) acima descritas\n",
    "- `calcula_custo` implementando a equação (2) acima\n",
    "- `metodo_do_gradiente`, utilizando as funções calcula_gradiente e calcula_custo\n",
    "\n",
    "Convenções:\n",
    "- A terminologia que usaremos em Python para variáveis contendo derivadas parciais é a serguinte: $\\frac{\\partial J(w,b)}{\\partial b}$  será `dj_db`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b457c",
   "metadata": {
    "id": "416b457c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### calcula_gradiente\n",
    "\n",
    "\n",
    "`calcula_gradiente`  implementa (4) e (5) acima e retorna $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29125631",
   "metadata": {
    "id": "29125631",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calcula_gradiente(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula o gradiente para Regressão Linear\n",
    "    Argumentos da função:\n",
    "      x (ndarray (m,)): Conjunto de dados com m amostras\n",
    "      y (ndarray (m,)): Valores alvo de saída\n",
    "      w,b (scalar)    : parâmetros do modelo\n",
    "    Retorna\n",
    "      dj_dw (scalar): O gradiente do custo em relação ao parâmetros w\n",
    "      dj_db (scalar): O gradiente do custo em relação ao parâmetros b\n",
    "     \"\"\"\n",
    "\n",
    "    # Número de amostras de treinamento\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = f_wb - y[i]\n",
    "\n",
    "        dj_db += dj_db_i\n",
    "        dj_db += dj_db_i\n",
    "\n",
    "        dj_dw += dj_dw_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1a1db",
   "metadata": {
    "id": "c6a1a1db",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c73a11",
   "metadata": {
    "id": "03c73a11",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testando nossa implementação da função calcula_gradiente\n",
    "\n",
    "\n",
    "Utilizando as linhas de código abaixo, teste diferentes valores para $b$ e $w$ com o objetivo de encontrar o valor mínimo da função custo $J(w,b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d486c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "037d486c",
    "outputId": "815da635-bedc-43b0-cad6-fd5ab66f5722",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b = 0\n",
    "w = 95\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_trein, y_trein, w, b)\n",
    "\n",
    "print(dj_dw)\n",
    "\n",
    "print(dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56af70db",
   "metadata": {
    "id": "56af70db",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Método do Gradiente\n",
    "\n",
    "Agora que o gradiente pode ser calculado, o Método do Gradiente, descrito na equação (3), é implementado abaixo por meio da função `metodo_do_gradiente`. Os detalhes da implementação estão descritos nos comentários. Abaixo, você vai utilizar essa função para encontrar os valores ótimos para $w$ e $b$ levando em conta o conjunto de dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebef798",
   "metadata": {
    "id": "3ebef798",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metodo_do_gradiente(x, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente):\n",
    "    \"\"\"\n",
    "    Aplica o Método do Gradiente para ajustar w,b. Atualiza w,b ao longo de\n",
    "    num_iters passos (iterações) assumindo uma taxa de aprendizado alpha\n",
    "\n",
    "    Argumentos da função:\n",
    "      x (ndarray (m,))  : Conjunto de dados com m amostras\n",
    "      y (ndarray (m,))  : Valores alvo de saída\n",
    "      w_in,b_in (scalar): valores iniciais para os parâmetros w,b\n",
    "      alpha (float):      taxa de aprendizado\n",
    "      num_iters (int):    número de iterações para o método\n",
    "      calcula_custo:      função responsável por calcular o custo\n",
    "      calcula_gradiente:  função responsável por calcular o gradiente\n",
    "\n",
    "    Retorna:\n",
    "      w (scalar): Valor atualizado para w após rodar o Método do Gradiente\n",
    "      b (scalar): Valor atualizado para b após rodar o Método do Gradiente\n",
    "      J_history (List): Contém o histórico dos valores de custo\n",
    "      p_history (list): Contém o histórico dos valores para [w,b]\n",
    "      \"\"\"\n",
    "\n",
    "    # Arrays que armazenam os valores históricos de J e w para cada iteração para que seja possível fazer gráfico depois\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calcula o gradiente usando a função calcula_gradiente\n",
    "        dj_dw, dj_db = calcula_gradiente(x, y, w , b)\n",
    "\n",
    "        # Atualiza os parâmetros w,b a partir do gradiente calculado\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "\n",
    "        # Salva o custo J para cada iteração\n",
    "        if i<100000:\n",
    "            J_history.append( calcula_custo(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Faz print em tempo real enquanto o Método do Gradiente estiver rodando\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteração {i:4}: Custo {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "\n",
    "    return w, b, J_history, p_history #retorna w,b e valores históricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56b234",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d56b234",
    "outputId": "6891085f-933e-4533-a685-a2bb924a32c3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inicialização de parâmetros\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# Parametrização do Método do Gradiente:\n",
    "iteracoes = 3000\n",
    "alpha = 1.0e-2\n",
    "# Roda o método do gradiente\n",
    "w_final, b_final, J_hist, p_hist = metodo_do_gradiente(x_trein ,y_trein, w_init, b_init, alpha,\n",
    "                                                    iteracoes, calcula_custo, calcula_gradiente)\n",
    "print(f\"(w,b) Encontrados pelo Método do Gradiente: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7866a",
   "metadata": {
    "id": "e1f7866a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe algumas características do Método do Gradiente:\n",
    "\n",
    "- O Custo começa elevado e decai rapidamente, conforme vimos na teoria.\n",
    "- As derivadas parciais, `dj_dw`, e `dj_db` também diminuem, inicialmente mais rapidamente e depois mais devagar. Isso acontece porque as derivadas parciais se aproximam de zero à medida com que nos aproximamos do mínimo da função"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b77602",
   "metadata": {
    "id": "f5b77602",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Custo versus iterações no Método do Gradiente\n",
    "\n",
    "Um gráfico do custo versus iterações é uma métrica interessante para verificarmos o progresso feito pelo método. O custo deve sempre decair ao longo das iterações. A mudança no custo é rápida inicialmente, portanto, é útil plotar as primeiras iterações numa escala diferente das últimas iterações. Observe as escalas usadas nos gráficos abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8148a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "5e8148a8",
    "outputId": "073866c9-124f-432d-a894-fe1dd94519b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# gráficos do custo versus iterações\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(15,6))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(500 + np.arange(len(J_hist[500:])), J_hist[500:])\n",
    "ax1.set_title(\"Custo vs. iteração(primeiras iterações)\");  ax2.set_title(\"Custo vs. iteração(últimas iterações)\")\n",
    "ax1.set_ylabel('Custo')            ;  ax2.set_ylabel('Custo')\n",
    "ax1.set_xlabel('iteração')  ;  ax2.set_xlabel('iteração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c974e",
   "metadata": {
    "id": "5c5c974e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Previsões\n",
    "\n",
    "\n",
    "Agora que você já descobriu os valores ótimos para os parâmetros $w$ e $b$, você pode usar o modelo para prever o valor de tensão do resistor para diferentes valores de corrente.\n",
    "\n",
    "Olhe os exemplos abaixo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b13f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "006b13f0",
    "outputId": "3ec89533-c5eb-4e0a-bd11-282d5fef41b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Para uma corrente de 1 A temos uma tensão esperada de {w_final*1.0 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 2.35 A temos uma tensão esperada de {w_final*2.35 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 3 A temos uma tensão esperada de {w_final*3.0 + b_final:0.1f} Volts\")\n",
    "\n",
    "x_test = np.linspace(10, 20, 10)\n",
    "messages = [f\"Para uma corrente de {x:.2f} A temos uma tensão esperada de {w_final*x + b_final:0.1f} Volts\" for x in x_test]\n",
    "messages = \"\\n\".join(messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc45f74",
   "metadata": {
    "id": "7dc45f74",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Parabéns!\n",
    "Você aprendeu o seguinte com esse código:\n",
    "- Detalhes importantes acerca do Método do Gradiente para uma variável\n",
    "- Desenvolveu uma rotina para o cálculo do gradiente\n",
    "- completou a rotina que aplica o Método do Gradiente\n",
    "- Utilizou o Método do Gradiente para encontrar os parâmetros ótimos para um modelo na forma de reta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71e3d3",
   "metadata": {},
   "source": [
    "## Calculando a reta extrapolando os valores do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_calc = np.linspace(1, 10, n_points)\n",
    "# Calculando a tensão estimada para o modelo para as correntes em x_trein\n",
    "\n",
    "noisy_signal = rng.uniform(-delta, delta, n_points)\n",
    "# descomentar a próxima linha, para ver o desempenho ante um ruído gaussiano\n",
    "# noisy_signal = rng.normal(mu, sigma, n_points) \n",
    "\n",
    "f_wb = w_final * x_calc + b_final\n",
    "y_calc =  w_truth * x_calc + b_truth + noisy_signal\n",
    "\n",
    "J = calcula_custo(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando a previsão feita pelo modelo\n",
    "plt.plot(x_calc, f_wb, c='b',label='Modelo do Método do Gradiente')\n",
    "\n",
    "# Plotando os dados medidos\n",
    "plt.scatter(x_calc, y_calc, marker='x', c='r',label='Valores dos dados')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('Tensão (em Volts)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('Corrente (em Àmperes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Custo J para esse conjunto de parâmetros w,b: {J:.04f}\")\n",
    "print(f\"dj_dw: {dj_dw:.4e}, dj_db: {dj_db:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores_para_w = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "valores_para_w = np.linspace(40, 60, 100, endpoint=True) # deixando a curva mais bonitinha\n",
    "\n",
    "qtdade_de_valores = valores_para_w.shape[0]\n",
    "\n",
    "J = np.zeros(qtdade_de_valores)\n",
    "\n",
    "for i in range(qtdade_de_valores):\n",
    "    w    = valores_para_w[i]\n",
    "    J[i] = calcula_custo(x_calc, y_calc, w, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando o custo em J para diferentes valores de w\n",
    "plt.plot(valores_para_w, J, c='b')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('J(w)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c513bd5",
   "metadata": {},
   "source": [
    "## Happy Hour - Implementando métodos alternativos para efeitos de comparação\n",
    "Algoritmos implementados:\n",
    "- RMSProp\n",
    "- Método de Newton\n",
    "- SGD (Gradiente Estocástico)\n",
    "- Gradiente Conjugado\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2414cc",
   "metadata": {},
   "source": [
    "### **Método RMSProp**\n",
    "\n",
    "O **RMSProp** (Root Mean Square Propagation) é um algoritmo de otimização baseado no método de gradiente descendente, projetado para resolver algumas limitações de técnicas mais simples, como o Gradiente Descendente Estocástico (SGD). Ele foi proposto por Geoffrey Hinton em uma palestra e é especialmente útil em problemas de redes neurais profundas, onde a convergência pode ser lenta ou inconstante.\n",
    "\n",
    "#### **Intuição**\n",
    "O RMSProp tenta lidar com dois problemas comuns no gradiente descendente:\n",
    "1. **Oscilações no gradiente**: O RMSProp reduz oscilações indesejadas em direções de alta curvatura da função de custo (frequentemente observadas ao longo de eixos de inclinação íngreme).\n",
    "2. **Taxa de aprendizado adaptativa**: Diferente de métodos clássicos, o RMSProp ajusta a taxa de aprendizado de forma dinâmica para cada parâmetro com base na magnitude recente dos gradientes.\n",
    "\n",
    "#### **Parte Matemática**\n",
    "No RMSProp, ao invés de usar diretamente o gradiente $\\nabla J(\\theta)$ para atualizar os parâmetros $\\theta$, o algoritmo introduz uma forma de controlar a magnitude dos passos com base na média quadrática móvel dos gradientes.\n",
    "\n",
    "Aqui estão as equações matemáticas que descrevem o algoritmo:\n",
    "\n",
    "1. **Cálculo da média exponencial dos quadrados dos gradientes**:\n",
    "   \n",
    "   $\n",
    "   E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2\n",
    "   $\n",
    "   Onde:\n",
    "   - $g_t$ é o gradiente no tempo $t$.\n",
    "   - $\\beta$ é um parâmetro de decaimento (para a maior parte das aplicações um valor de 0.9).\n",
    "   - $E[g^2]_t$ é a média exponencial do quadrado dos gradientes até o tempo $t$.\n",
    "\n",
    "3. **Atualização dos parâmetros**:\n",
    "\n",
    "   $\n",
    "   \\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "   $\n",
    "   Onde:\n",
    "   - $\\alpha$ é a taxa de aprendizado.\n",
    "   - $\\epsilon$ é um valor pequeno adicionado para evitar divisões por zero (tipicamente $10^{-8}$).\n",
    "   - A divisão pelo termo $\\sqrt{E[g^2]_t + \\epsilon}$ ajusta a taxa de aprendizado para cada parâmetro de acordo com a magnitude recente de seus gradientes.\n",
    "\n",
    "#### **Vantagens do RMSProp**\n",
    "\n",
    "   - **Taxa de aprendizado adaptativa:** O RMSProp ajusta a taxa de aprendizado para cada parâmetro individualmente, o que facilita a convergência e ajuda a evitar grandes oscilações.\n",
    "   - **Bom desempenho em redes profundas:** Funciona bem em redes neurais profundas, onde o gradiente pode variar bastante para diferentes parâmetros.\n",
    "#### **Desvantagens do RMSProp**\n",
    "\n",
    "   - **Dificuldade em Ajustar a Taxa de Decaimento:** O parâmetro de decaimento exponencial, que controla o quanto o gradiente é suavizado ao longo do tempo, pode ser sensível e difícil de ajustar corretamente para diferentes problemas. Uma escolha inadequada pode levar a uma convergência lenta ou até impedir a convergência.\n",
    "\n",
    "   - **Estagnação em Mínimos Locais:** Como o RMSProp ajusta a taxa de aprendizado com base nas médias dos gradientes passados, pode acabar ficando preso em mínimos locais ou em regiões onde os gradientes são muito pequenos.\n",
    "\n",
    "   - **Pode Precisar de Ajuste Adicional:** Embora o RMSProp forneça taxas de aprendizado adaptativas, ainda pode ser necessário ajustar o valor inicial da taxa de aprendizado global para obter bons resultados em alguns problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041008f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(x, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente, beta=0.9, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Aplica o método RMSProp para ajustar w, b.\n",
    "\n",
    "    Argumentos da função:\n",
    "      x (ndarray (m,))  : Conjunto de dados com m amostras\n",
    "      y (ndarray (m,))  : Valores alvo de saída\n",
    "      w_in, b_in (scalar): Valores iniciais para os parâmetros w e b\n",
    "      alpha (float)     : Taxa de aprendizado\n",
    "      num_iters (int)   : Número de iterações\n",
    "      calcula_custo     : Função para calcular o custo\n",
    "      calcula_gradiente : Função para calcular o gradiente\n",
    "      beta (float)      : Fator de decaimento para RMSProp (geralmente próximo de 0.9)\n",
    "      epsilon (float)   : Pequeno valor para evitar divisão por zero no cálculo do gradiente (geralmente 1e-8)\n",
    "\n",
    "    Retorna:\n",
    "      w (scalar)        : Valor atualizado de w\n",
    "      b (scalar)        : Valor atualizado de b\n",
    "      J_history (list)  : Histórico dos valores de custo\n",
    "      p_history (list)  : Histórico dos valores para [w, b]\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar variáveis\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    # Inicialização de acumuladores de gradiente\n",
    "    v_dw = 0\n",
    "    v_db = 0\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calcula o gradiente\n",
    "        dj_dw, dj_db = calcula_gradiente(x, y, w, b)\n",
    "\n",
    "        # Atualiza os acumuladores RMSProp para w e b\n",
    "        v_dw = beta * v_dw + (1 - beta) * (dj_dw**2)\n",
    "        v_db = beta * v_db + (1 - beta) * (dj_db**2)\n",
    "\n",
    "        # Atualiza os parâmetros w e b\n",
    "        w = w - alpha * dj_dw / (np.sqrt(v_dw) + epsilon)\n",
    "        b = b - alpha * dj_db / (np.sqrt(v_db) + epsilon)\n",
    "\n",
    "        # Armazena o custo para cada iteração\n",
    "        J_history.append(calcula_custo(x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "\n",
    "        # Exibe o progresso em intervalos regulares\n",
    "        if i % (num_iters // 10) == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteração {i:4}: Custo {J_history[-1]:0.2e}, w: {w:0.3f}, b: {b:0.3f}\")\n",
    "\n",
    "    return w, b, J_history, p_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8948b00",
   "metadata": {},
   "source": [
    "### **Método de Newton**\n",
    "\n",
    "O **método de Newton** é uma técnica de otimização numérica que utiliza informações de segunda ordem (a Hessiana, ou seja, a matriz de derivadas segundas) para encontrar o mínimo de uma função. Diferente do gradiente descendente, que usa apenas a inclinação da função para ajustar os parâmetros, o método de Newton também leva em conta a curvatura, proporcionando um caminho de otimização mais eficiente, especialmente em funções convexas.\n",
    "\n",
    "#### **Intuição**\n",
    "\n",
    "A ideia principal do método de Newton é aproximar a função de custo $J(\\theta)$ por uma série de Taylor de segunda ordem. A aproximação quadrática da função de custo ao redor do ponto atual $\\theta$ pode ser usada para encontrar a próxima iteração, onde o mínimo pode ser atingido mais rapidamente.\n",
    "\n",
    "A atualização dos parâmetros é dada pela fórmula:\n",
    "\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t - H^{-1} \\nabla J(\\theta)\n",
    "$\n",
    "\n",
    "Onde:\n",
    "- $\\theta_t$ é o vetor de parâmetros na iteração $t$,\n",
    "- $\\nabla J(\\theta)$ é o gradiente da função de custo com relação a $\\theta$,\n",
    "- $H$ é a **matriz Hessiana**, que contém as derivadas segundas da função de custo com relação a $\\theta$.\n",
    "\n",
    "#### **Parte Matemática**\n",
    "\n",
    "##### **Gradiente**\n",
    "\n",
    "O gradiente $\\nabla J(\\theta)$ é o vetor de derivadas parciais da função de custo $J$ em relação aos parâmetros $\\theta$:\n",
    "$\n",
    "\\nabla J(\\theta) = \\left[ \\frac{\\partial J}{\\partial \\theta_1}, \\frac{\\partial J}{\\partial \\theta_2}, ..., \\frac{\\partial J}{\\partial \\theta_n} \\right]\n",
    "$\n",
    "\n",
    "##### **Hessiana**\n",
    "\n",
    "A matriz Hessiana $H$ é uma matriz quadrada de ordem $n \\times n$ que contém todas as derivadas segundas da função de custo com respeito aos parâmetros $\\theta$:\n",
    "\n",
    "$\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 J(w, b)}{\\partial w^2} & \\frac{\\partial^2 J(w, b)}{\\partial w \\partial b} \\\\\n",
    "\\frac{\\partial^2 J(w, b)}{\\partial b \\partial w} & \\frac{\\partial^2 J(w, b)}{\\partial b^2}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Ela captura a curvatura da função de custo, e sua inversa permite ajustar os passos com mais precisão, levando em consideração a geometria local da função.\n",
    "\n",
    "#### **Vantagens do Método de Newton**\n",
    "\n",
    "- **Convergência Rápida**: Quando a função de custo é convexa, o método de Newton pode convergir muito mais rápido que o gradiente descendente, pois leva em conta a curvatura.\n",
    "- **Passos Adaptativos**: A atualização dos parâmetros é baseada na inversa da Hessiana, o que ajusta automaticamente o tamanho dos passos dependendo da curvatura local.\n",
    "\n",
    "#### **Desvantagens do Método de Newton**\n",
    "\n",
    "- **Custo Computacional**: O cálculo da matriz Hessiana e sua inversa pode ser computacionalmente caro, especialmente para problemas com alta dimensionalidade.\n",
    "- **Não Adequado para Funções Não Convexas**: Se a função de custo não for convexa, o método de Newton pode falhar ao encontrar um mínimo global, convergindo para mínimos locais ou pontos de sela.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calcula_hessiana(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula a matriz Hessiana da função de custo para regressão linear.\n",
    "\n",
    "    Argumentos:\n",
    "      x (ndarray (m,)) : Conjunto de dados com m amostras\n",
    "      y (ndarray (m,)) : Valores alvo de saída\n",
    "      w (scalar)       : Parâmetro w da regressão linear\n",
    "      b (scalar)       : Parâmetro b da regressão linear\n",
    "\n",
    "    Retorna:\n",
    "      H (ndarray (2,2)): Matriz Hessiana 2x2\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    # Calcula os elementos da matriz Hessiana\n",
    "    H_ww = (1/m) * np.sum(x ** 2)  # ∂²J/∂w²\n",
    "    H_wb = (1/m) * np.sum(x)       # ∂²J/∂w∂b ou ∂²J/∂b∂w\n",
    "    H_bb = 1                       # ∂²J/∂b²\n",
    "\n",
    "    # Matriz Hessiana 2x2\n",
    "    H = np.array([[H_ww, H_wb],\n",
    "                  [H_wb, H_bb]])\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def metodo_de_newton(x, y, w_in, b_in, num_iters, calcula_custo, calcula_gradiente, calcula_hessiana, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Aplica o Método de Newton para ajustar w, b.\n",
    "\n",
    "    Argumentos da função:\n",
    "      x (ndarray (m,))  : Conjunto de dados com m amostras\n",
    "      y (ndarray (m,))  : Valores alvo de saída\n",
    "      w_in, b_in (scalar): Valores iniciais para os parâmetros w e b\n",
    "      num_iters (int)   : Número de iterações máximas\n",
    "      calcula_custo     : Função para calcular o custo\n",
    "      calcula_gradiente : Função para calcular o gradiente\n",
    "      calcula_hessiana  : Função para calcular a Hessiana\n",
    "      tol (float)       : Tolerância para a convergência\n",
    "\n",
    "    Retorna:\n",
    "      w (scalar)        : Valor atualizado de w\n",
    "      b (scalar)        : Valor atualizado de b\n",
    "      J_history (list)  : Histórico dos valores de custo\n",
    "      p_history (list)  : Histórico dos valores para [w, b]\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar variáveis\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Calcula o gradiente e a Hessiana\n",
    "        dj_dw, dj_db = calcula_gradiente(x, y, w, b)\n",
    "        H = calcula_hessiana(x, y, w, b)\n",
    "        \n",
    "        # Vetor de gradientes\n",
    "        grad = np.array([dj_dw, dj_db])\n",
    "        \n",
    "        # Montar a Hessiana como uma matriz 2x2\n",
    "        H_inv = np.linalg.inv(H)  # Inversão da matriz Hessiana\n",
    "\n",
    "        # Atualiza os parâmetros w e b\n",
    "        update = np.dot(H_inv, grad)  # H^(-1) * grad\n",
    "        w = w - update[0]\n",
    "        b = b - update[1]\n",
    "\n",
    "        # Armazena o custo e os parâmetros\n",
    "        J_history.append(calcula_custo(x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "\n",
    "        # Verifica a convergência pelo tamanho do gradiente\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Convergência atingida na iteração {i}.\")\n",
    "            break\n",
    "\n",
    "        # Exibe o progresso em intervalos regulares\n",
    "        if i % (num_iters // 10) == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteração {i:4}: Custo {J_history[-1]:0.2e}, w: {w:0.3f}, b: {b:0.3f}\")\n",
    "\n",
    "    return w, b, J_history, p_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371d3f8",
   "metadata": {},
   "source": [
    "### **Gradiente Descendente Estocástico (SGD)**\n",
    "\n",
    "O **Gradiente Descendente Estocástico (SGD)** é uma variante do método de gradiente descendente que atualiza os parâmetros do modelo utilizando apenas um subconjunto (ou uma única amostra) do conjunto de dados a cada iteração. Essa abordagem pode resultar em uma convergência mais rápida, especialmente em grandes conjuntos de dados.\n",
    "\n",
    "#### **Intuição**\n",
    "\n",
    "O gradiente descendente tradicional utiliza a média de todos os gradientes das amostras para atualizar os parâmetros, o que pode ser computacionalmente custoso para grandes conjuntos de dados. O SGD, por outro lado, escolhe aleatoriamente uma amostra ou um mini-lote de amostras para calcular a atualização do parâmetro, permitindo que o algoritmo comece a atualizar os parâmetros imediatamente, o que pode acelerar a convergência e ajudar a escapar de mínimos locais.\n",
    "\n",
    "##### **Parte Matemática**\n",
    "\n",
    "A atualização dos parâmetros $\\theta$ no SGD é dada pela seguinte fórmula:\n",
    "\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t; x^{(i)}, y^{(i)})\n",
    "$\n",
    "\n",
    "Onde:\n",
    "- $\\theta_t$ é o vetor de parâmetros na iteração $t$,\n",
    "- $\\alpha$ é a taxa de aprendizado,\n",
    "- $\\nabla J(\\theta_t; x^{(i)}, y^{(i)})$ é o gradiente da função de custo calculado para a amostra $(x^{(i)}, y^{(i)})$.\n",
    "\n",
    "#### **Vantagens do SGD**\n",
    "\n",
    "1. **Eficiência Computacional**: O SGD é muito mais rápido que o gradiente descendente padrão em grandes conjuntos de dados, pois utiliza apenas uma amostra (ou um pequeno lote) para cada atualização.\n",
    "2. **Capacidade de Escapar de Mínimos Locais**: A natureza estocástica do SGD pode ajudar a escapar de mínimos locais, uma característica especialmente útil em funções de custo não convexas.\n",
    "\n",
    "#### **Desvantagens do SGD**\n",
    "\n",
    "1. **Oscilações**: As atualizações estocásticas podem causar oscilações na função de custo, dificultando a convergência em um mínimo global.\n",
    "2. **Escolha da Taxa de Aprendizado**: A escolha da taxa de aprendizado pode ser crítica; uma taxa muito alta pode levar a uma divergência, enquanto uma taxa muito baixa pode resultar em uma convergência muito lenta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sgd(x, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente):\n",
    "    \"\"\"\n",
    "    Aplica o Método do Gradiente Estocástico (SGD) para ajustar w, b.\n",
    "\n",
    "    Argumentos da função:\n",
    "      x (ndarray (m,))  : Conjunto de dados com m amostras\n",
    "      y (ndarray (m,))  : Valores alvo de saída\n",
    "      w_in, b_in (scalar): Valores iniciais para os parâmetros w e b\n",
    "      alpha (float)     : Taxa de aprendizado\n",
    "      num_iters (int)   : Número de iterações\n",
    "      calcula_custo     : Função para calcular o custo\n",
    "      calcula_gradiente : Função para calcular o gradiente\n",
    "\n",
    "    Retorna:\n",
    "      w (scalar)        : Valor atualizado de w\n",
    "      b (scalar)        : Valor atualizado de b\n",
    "      J_history (list)  : Histórico dos valores de custo\n",
    "      p_history (list)  : Histórico dos valores para [w, b]\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar variáveis\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    m = len(x)  # Número de amostras\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # Embaralhar os dados a cada iteração\n",
    "        indices = np.random.permutation(m)\n",
    "        x_shuffled = x[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # Atualiza w e b para cada exemplo de treinamento\n",
    "        for j in range(m):\n",
    "            # Seleciona uma única amostra\n",
    "            x_i = x_shuffled[j:j+1]\n",
    "            y_i = y_shuffled[j:j+1]\n",
    "\n",
    "            # Calcula o gradiente para essa amostra\n",
    "            dj_dw, dj_db = calcula_gradiente(x_i, y_i, w, b)\n",
    "\n",
    "            # Atualiza os parâmetros w e b\n",
    "            w = w - alpha * dj_dw\n",
    "            b = b - alpha * dj_db\n",
    "\n",
    "        # Armazena o custo após a passagem por todo o conjunto de dados\n",
    "        J_history.append(calcula_custo(x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "\n",
    "        # Exibe o progresso em intervalos regulares\n",
    "        if i % (num_iters // 10) == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteração {i:4}: Custo {J_history[-1]:0.2e}, w: {w:0.3f}, b: {b:0.3f}\")\n",
    "\n",
    "    return w, b, J_history, p_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd58dd",
   "metadata": {},
   "source": [
    "### **Método de Gradiente Conjugado**\n",
    "\n",
    "O **Método de Gradiente Conjugado** é uma técnica eficiente para resolver sistemas de equações lineares e minimizar funções quadráticas. Ele é amplamente utilizado em otimização, especialmente quando o número de parâmetros é grande, pois evita a necessidade de calcular a matriz Hessiana (que pode ser muito custosa em termos computacionais).\n",
    "\n",
    "Neste caso, aplicamos o **Gradiente Conjugado** para ajustar os parâmetros `w` e `b` de um modelo de regressão linear. O método explora as propriedades da função de custo para determinar a direção ótima de descida a cada iteração, buscando melhorar a velocidade de convergência em comparação ao gradiente descendente comum.\n",
    "\n",
    "#### **Descrição do Algoritmo**\n",
    "\n",
    "1. **Inicialização**:\n",
    "    - Definimos os valores iniciais de `w` e `b` como `w_in` e `b_in`, que serão ajustados pelo algoritmo.\n",
    "    - Os gradientes anteriores (`g_prev_w`, `g_prev_b`) são inicializados como `0`, assim como as direções conjugadas (`p_w`, `p_b`).\n",
    "\n",
    "2. **Cálculo do Gradiente**:\n",
    "    - Em cada iteração, calculamos o gradiente da função de custo em relação a `w` e `b` usando a função `calcula_gradiente`.\n",
    "\n",
    "3. **Critério de Parada**:\n",
    "    - Se a norma do gradiente for menor que um limiar definido (`tol`), o algoritmo para, indicando que a convergência foi atingida.\n",
    "\n",
    "4. **Atualização da Direção Conjugada**:\n",
    "    - Na primeira iteração, a direção de descida é definida como o negativo do gradiente.\n",
    "    - Em iterações subsequentes, calculamos o fator \\( \\beta_t \\) (Polak-Ribiére), que ajusta a nova direção de descida em função da direção anterior, de forma que as direções permaneçam conjugadas.\n",
    "\n",
    "5. **Busca Linear e Atualização**:\n",
    "    - A cada iteração, atualizamos `w` e `b` na direção conjugada calculada, com um passo de tamanho \\( \\alpha \\) (aqui, um valor fixo de 0.01, mas pode ser adaptado).\n",
    "\n",
    "6. **Histórico de Custo**:\n",
    "    - Armazenamos o valor do custo em cada iteração para monitorar o progresso da otimização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradiente_conjugado(x, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Método de Gradiente Conjugado para ajuste de w e b.\n",
    "    \n",
    "    Argumentos:\n",
    "      x (ndarray (m,))  : Conjunto de dados com m amostras\n",
    "      y (ndarray (m,))  : Valores alvo de saída\n",
    "      w_in, b_in (scalar): Valores iniciais para os parâmetros w e b\n",
    "      alpha (float): tamanho de passo ou taxa de aprendizado\n",
    "      num_iters (int)   : Número de iterações\n",
    "      calcula_custo: Função para calcular o custo.\n",
    "      calcula_gradiente: Função para calcular o gradiente.\n",
    "      tol (float): Tolerância para critério de parada.\n",
    "\n",
    "    Retorna:\n",
    "      w (scalar)        : Valor atualizado de w\n",
    "      b (scalar)        : Valor atualizado de b\n",
    "      J_history (list)  : Histórico dos valores de custo\n",
    "      p_history (list)  : Histórico dos valores para [w, b]\n",
    "    \"\"\"\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    \n",
    "    g_prev_w, g_prev_b = 0, 0\n",
    "    p_w, p_b = 0, 0\n",
    "    \n",
    "    for t in range(1, num_iters + 1):\n",
    "        # Calcula o gradiente\n",
    "        dj_dw, dj_db = calcula_gradiente(x, y, w, b)\n",
    "        \n",
    "        # Critério de parada\n",
    "        if np.linalg.norm(np.array([dj_dw, dj_db])) < tol:\n",
    "            print(f\"Convergência atingida na iteração {t}.\")\n",
    "            break\n",
    "        \n",
    "        # Se for a primeira iteração, inicializa p como o negativo do gradiente\n",
    "        if t == 1:\n",
    "            p_w = -dj_dw\n",
    "            p_b = -dj_db\n",
    "        else:\n",
    "            # Computa beta_t usando o método Polak-Ribiére\n",
    "            beta_t = (dj_dw**2 + dj_db**2) / (g_prev_w**2 + g_prev_b**2)\n",
    "            p_w = -dj_dw + beta_t * p_w\n",
    "            p_b = -dj_db + beta_t * p_b\n",
    "        \n",
    "        # Realiza a atualização (uma simples linha de busca com passo fixo pode ser usada)\n",
    "        w = w + alpha * p_w\n",
    "        b = b + alpha * p_b\n",
    "        \n",
    "        # Calcula o custo e armazena no histórico\n",
    "        J_history.append(calcula_custo(x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "        \n",
    "        # Atualiza o gradiente anterior\n",
    "        g_prev_w = dj_dw\n",
    "        g_prev_b = dj_db\n",
    "        \n",
    "        # Exibe o progresso em intervalos regulares\n",
    "        if t % (num_iters // 10) == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteração {t:4}: Custo {J_history[-1]:0.2e}, w: {w:0.3f}, b: {b:0.3f}\")\n",
    "    \n",
    "    return w, b, J_history, p_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a0e27",
   "metadata": {},
   "source": [
    "### **Algoritmo Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "O algoritmo **Adam** combina as ideias dos métodos de **Momentum** e **RMSProp** para realizar uma atualização adaptativa dos parâmetros durante a otimização. Ele ajusta a taxa de aprendizado de cada parâmetro individualmente, com base em estimativas de momentos de primeira e segunda ordem.\n",
    "\n",
    "#### **Etapas do Algoritmo**\n",
    "\n",
    "1. **Inicialização**:\n",
    "   - Inicializa os momentos $m_w, m_b, v_w, v_b$ como zeros, onde:\n",
    "     - $m_w, m_b$ são as médias móveis exponenciais dos gradientes para os parâmetros $w$ e $b$.\n",
    "     - $v_w, v_b$ são as médias móveis exponenciais dos quadrados dos gradientes.\n",
    "   - Define os hiperparâmetros: taxa de aprendizado $\\alpha$, decaimentos $\\beta_1, \\beta_2$ e um pequeno valor $\\epsilon$ para evitar divisão por zero (estabilização numérica).\n",
    "\n",
    "2. **Atualização dos Momentos**:\n",
    "3. \n",
    "   - A cada iteração, atualiza as médias móveis dos gradientes:\n",
    "    \n",
    "     $\n",
    "     m_w = \\beta_1 m_w + (1 - \\beta_1) \\cdot \\nabla J(w)\n",
    "     $\n",
    "\n",
    "     $\n",
    "     m_b = \\beta_1 m_b + (1 - \\beta_1) \\cdot \\nabla J(b)\n",
    "     $\n",
    "     \n",
    "   - Atualiza as médias móveis dos quadrados dos gradientes:\n",
    "\n",
    "     $\n",
    "     v_w = \\beta_2 v_w + (1 - \\beta_2) \\cdot \\left( \\nabla J(w) \\right)^2\n",
    "     $\n",
    "\n",
    "     $\n",
    "     v_b = \\beta_2 v_b + (1 - \\beta_2) \\cdot \\left( \\nabla J(b) \\right)^2\n",
    "     $\n",
    "\n",
    "4. **Correção de Bias**:\n",
    "   - Como $m_w$, $m_b$, $v_w$ e $v_b$ estão inicialmente biasados em direção a zero nas primeiras iterações, faz-se a correção de bias:\n",
    "\n",
    "     $\n",
    "     m_w^{\\text{corrigido}} = \\frac{m_w}{1 - \\beta_1^t}, \\quad m_b^{\\text{corrigido}} = \\frac{m_b}{1 - \\beta_1^t}\n",
    "     $\n",
    "\n",
    "     $\n",
    "     v_w^{\\text{corrigido}} = \\frac{v_w}{1 - \\beta_2^t}, \\quad v_b^{\\text{corrigido}} = \\frac{v_b}{1 - \\beta_2^t}\n",
    "     $\n",
    "\n",
    "5. **Atualização dos Parâmetros**:\n",
    "   - Finalmente, os parâmetros $w$ e $b$ são atualizados de acordo com as médias corrigidas e os quadrados dos gradientes:\n",
    "\n",
    "     $\n",
    "     w = w - \\alpha \\cdot \\frac{m_w^{\\text{corrigido}}}{\\sqrt{v_w^{\\text{corrigido}}} + \\epsilon}\n",
    "     $\n",
    "\n",
    "     $\n",
    "     b = b - \\alpha \\cdot \\frac{m_b^{\\text{corrigido}}}{\\sqrt{v_b^{\\text{corrigido}}} + \\epsilon}\n",
    "     $\n",
    "\n",
    "### Hiperparâmetros do Adam\n",
    "\n",
    "- $\\alpha$ : Taxa de aprendizado (comumente $0.001$).\n",
    "- $\\beta_1$ : Exponencial para a média dos gradientes (geralmente $0.9$).\n",
    "- $\\beta_2$ : Exponencial para a média dos gradientes ao quadrado (geralmente $0.999$).\n",
    "- $\\epsilon$ : Pequeno valor para evitar divisão por zero (geralmente $10^{-8}$).\n",
    "\n",
    "### Vantagens do Adam\n",
    "\n",
    "- **Taxa de aprendizado adaptativa**: O Adam ajusta automaticamente a taxa de aprendizado para cada parâmetro, o que torna o algoritmo mais robusto em diferentes tipos de problemas.\n",
    "- **Combinação de Momentos**: Aproveita a média dos gradientes (momentum) e as variâncias dos gradientes (RMSProp), combinando os benefícios dos dois métodos.\n",
    "- **Desempenho em Redes Neurais**: Funciona bem em redes neurais profundas e problemas onde o gradiente pode variar muito entre parâmetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(x, y, w_in, b_in, alpha, beta1, beta2, epsilon, num_iters, calcula_custo, calcula_gradiente):\n",
    "    \"\"\"\n",
    "    Aplica o algoritmo Adam para otimização de w e b.\n",
    "\n",
    "    Argumentos:\n",
    "      x (ndarray): Conjunto de dados com m amostras e n características.\n",
    "      y (ndarray): Valores alvo de saída.\n",
    "      w_in (ndarray): Valores iniciais para os parâmetros w.\n",
    "      b_in (scalar): Valor inicial para o parâmetro b.\n",
    "      alpha (float): Taxa de aprendizado.\n",
    "      beta1 (float): Exponencial para a média dos gradientes.\n",
    "      beta2 (float): Exponencial para a média dos gradientes ao quadrado.\n",
    "      epsilon (float): Pequeno valor para evitar divisão por zero.\n",
    "      num_iters (int): Número total de iterações.\n",
    "      calcula_custo: Função que calcula o custo.\n",
    "      calcula_gradiente: Função que calcula o gradiente.\n",
    "\n",
    "    Retorna:\n",
    "      w (ndarray): Valor otimizado de w.\n",
    "      b (scalar): Valor otimizado de b.\n",
    "      J_history (list): Histórico dos valores de custo.\n",
    "    \"\"\"\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    m_w, v_w = np.zeros_like(w), np.zeros_like(w)  # Inicialização dos momentos de w\n",
    "    m_b, v_b = 0, 0  # Inicialização dos momentos de b\n",
    "\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "\n",
    "    for t in range(1, num_iters + 1):\n",
    "        # Calcula o gradiente\n",
    "        dj_dw, dj_db = calcula_gradiente(x, y, w, b)\n",
    "\n",
    "        # Atualiza as médias e variâncias dos gradientes (momentos)\n",
    "        m_w = beta1 * m_w + (1 - beta1) * dj_dw\n",
    "        m_b = beta1 * m_b + (1 - beta1) * dj_db\n",
    "\n",
    "        v_w = beta2 * v_w + (1 - beta2) * (dj_dw ** 2)\n",
    "        v_b = beta2 * v_b + (1 - beta2) * (dj_db ** 2)\n",
    "\n",
    "        # Correções de bias\n",
    "        m_w_corrigido = m_w / (1 - beta1 ** t)\n",
    "        m_b_corrigido = m_b / (1 - beta1 ** t)\n",
    "        v_w_corrigido = v_w / (1 - beta2 ** t)\n",
    "        v_b_corrigido = v_b / (1 - beta2 ** t)\n",
    "\n",
    "        # Atualiza os parâmetros\n",
    "        w -= alpha * (m_w_corrigido / (np.sqrt(v_w_corrigido) + epsilon))\n",
    "        b -= alpha * (m_b_corrigido / (np.sqrt(v_b_corrigido) + epsilon))\n",
    "\n",
    "        # Armazena o custo atual\n",
    "        J_history.append(calcula_custo(x, y, w, b))\n",
    "        p_history.append([w, b])\n",
    "\n",
    "        # Exibe o progresso em intervalos regulares\n",
    "        if t % (num_iters // 10) == 0 or i == num_iters - 1:\n",
    "            print(f\"Iteração {t:4}: Custo {J_history[-1]:0.2e}, w: {w:0.3f}, b: {b:0.3f}\")\n",
    "\n",
    "    return w, b, J_history, p_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541d809",
   "metadata": {},
   "source": [
    "### Parte Prática\n",
    "\n",
    "#### Rodando o RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteracoes = 3000\n",
    "# meio dificil lidar com a taxa de aprendizado deste algoritmo\n",
    "alpha = 0.085\n",
    "# Roda o método do gradiente\n",
    "w_final, b_final, J_hist, p_hist = rmsprop(x_trein ,y_trein, w_init, b_init, alpha,\n",
    "                                                    iteracoes, calcula_custo, calcula_gradiente)\n",
    "print(f\"(w,b) Encontrados pelo Método RMSProp: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef1979",
   "metadata": {},
   "source": [
    "#### Custo versus iterações no Método RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos do custo versus iterações\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(15,6))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(500 + np.arange(len(J_hist[500:])), J_hist[500:])\n",
    "ax1.set_title(\"Custo vs. iteração(primeiras iterações)\");  ax2.set_title(\"Custo vs. iteração(últimas iterações)\")\n",
    "ax1.set_ylabel('Custo')            ;  ax2.set_ylabel('Custo')\n",
    "ax1.set_xlabel('iteração')  ;  ax2.set_xlabel('iteração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d1b46",
   "metadata": {},
   "source": [
    "#### Previsões Utilizando os parâmetros encontrados pelo Método RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e31692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Para uma corrente de 1 A temos uma tensão esperada de {w_final*1.0 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 2.35 A temos uma tensão esperada de {w_final*2.35 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 3 A temos uma tensão esperada de {w_final*3.0 + b_final:0.1f} Volts\")\n",
    "\n",
    "x_test = np.linspace(10, 20, 10)\n",
    "messages = [f\"Para uma corrente de {x:.2f} A temos uma tensão esperada de {w_final*x + b_final:0.1f} Volts\" for x in x_test]\n",
    "messages = \"\\n\".join(messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17136f0",
   "metadata": {},
   "source": [
    "#### Calculando a reta extrapolando os valores do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d01d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wb = w_final * x_calc + b_final\n",
    "\n",
    "J = calcula_custo(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando a previsão feita pelo modelo\n",
    "plt.plot(x_calc, f_wb, c='b',label='Modelo do Método RMSProp')\n",
    "\n",
    "# Plotando os dados medidos\n",
    "plt.scatter(x_calc, y_calc, marker='x', c='r',label='Valores dos dados')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('Tensão (em Volts)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('Corrente (em Àmperes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Custo J para esse conjunto de parâmetros w,b: {J:.04f}\")\n",
    "print(f\"dj_dw: {dj_dw:.4e}, dj_db: {dj_db:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211241dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores_para_w = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "valores_para_w = np.linspace(40, 60, 100, endpoint=True) # deixando a curva mais bonitinha\n",
    "\n",
    "qtdade_de_valores = valores_para_w.shape[0]\n",
    "\n",
    "J = np.zeros(qtdade_de_valores)\n",
    "\n",
    "for i in range(qtdade_de_valores):\n",
    "    w    = valores_para_w[i]\n",
    "    J[i] = calcula_custo(x_calc, y_calc, w, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando o custo em J para diferentes valores de w\n",
    "plt.plot(valores_para_w, J, c='b')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('J(w)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90cd8a-79b2-4b5e-bd55-a8f2de500e62",
   "metadata": {},
   "source": [
    "#### Rodando o Método de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882e845-769f-49c4-ae1a-a69c25a771d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteracoes = 3000\n",
    "# o método é deveras sensível ao ponto inicial - este deve ser próximo do ponto de interesse\n",
    "w_inicial = 49.5\n",
    "b_inicial = 1.95\n",
    "# Roda o método do gradiente\n",
    "w_final, b_final, J_hist, p_hist = metodo_de_newton(x_trein ,y_trein, w_inicial, b_inicial,                                                    iteracoes, calcula_custo, calcula_gradiente, calcula_hessiana)\n",
    "print(f\"(w,b) Encontrados pelo Método de Newton: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e9b5e-48eb-49a3-b966-90bfcc4b99c9",
   "metadata": {},
   "source": [
    "#### Custo versus iterações no Método De Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663296b-cbe9-43ff-8a0e-0d8cfe491045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos do custo versus iterações\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(15,6))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(500 + np.arange(len(J_hist[500:])), J_hist[500:])\n",
    "ax1.set_title(\"Custo vs. iteração(primeiras iterações)\");  ax2.set_title(\"Custo vs. iteração(últimas iterações)\")\n",
    "ax1.set_ylabel('Custo')            ;  ax2.set_ylabel('Custo')\n",
    "ax1.set_xlabel('iteração')  ;  ax2.set_xlabel('iteração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b151253-c793-4da6-bf4d-9f5e6f6cbe65",
   "metadata": {},
   "source": [
    "#### Previsões Utilizando os parâmetros encontrados pelo Método de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ec50e-0a9c-463d-847d-c24d9df084be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Para uma corrente de 1 A temos uma tensão esperada de {w_final*1.0 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 2.35 A temos uma tensão esperada de {w_final*2.35 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 3 A temos uma tensão esperada de {w_final*3.0 + b_final:0.1f} Volts\")\n",
    "\n",
    "x_test = np.linspace(10, 20, 10)\n",
    "messages = [f\"Para uma corrente de {x:.2f} A temos uma tensão esperada de {w_final*x + b_final:0.1f} Volts\" for x in x_test]\n",
    "messages = \"\\n\".join(messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fda95-f69e-4af9-a94c-f905e6a9fa84",
   "metadata": {},
   "source": [
    "#### Calculando a reta extrapolando os valores do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16109f20-ca39-4f81-a162-7efe16cdc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wb = w_final * x_calc + b_final\n",
    "\n",
    "J = calcula_custo(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando a previsão feita pelo modelo\n",
    "plt.plot(x_calc, f_wb, c='b',label='Modelo do Método de Newton')\n",
    "\n",
    "# Plotando os dados medidos\n",
    "plt.scatter(x_calc, y_calc, marker='x', c='r',label='Valores dos dados')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('Tensão (em Volts)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('Corrente (em Àmperes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Custo J para esse conjunto de parâmetros w,b: {J:.04f}\")\n",
    "print(f\"dj_dw: {dj_dw:.4e}, dj_db: {dj_db:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07178e08-a8ce-43a5-9f2e-178d072bf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores_para_w = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "valores_para_w = np.linspace(40, 60, 100, endpoint=True) # deixando a curva mais bonitinha\n",
    "\n",
    "qtdade_de_valores = valores_para_w.shape[0]\n",
    "\n",
    "J = np.zeros(qtdade_de_valores)\n",
    "\n",
    "for i in range(qtdade_de_valores):\n",
    "    w    = valores_para_w[i]\n",
    "    J[i] = calcula_custo(x_calc, y_calc, w, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando o custo em J para diferentes valores de w\n",
    "plt.plot(valores_para_w, J, c='b')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('J(w)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac3438-a897-4eff-9443-eeaa9ff1d712",
   "metadata": {},
   "source": [
    "#### Rodando o SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae4018-3f1d-4782-91e8-d224e1112169",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteracoes = 3000\n",
    "# SGD trabalha melhor com taxas de aprendizado pequenas\n",
    "alpha = 9e-3 # otimizado pelo método do olhômetro e VC (Vai Cavalo!!!)\n",
    "w_inicial = 0\n",
    "b_inicial = 0\n",
    "# Roda o método do gradiente\n",
    "w_final, b_final, J_hist, p_hist = sgd(x_trein ,y_trein, w_inicial, b_inicial, alpha,\n",
    "                                                    iteracoes, calcula_custo, calcula_gradiente)\n",
    "print(f\"(w,b) Encontrados pelo Método SGD: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c18c10-f7fb-4858-b15e-f13844e332e7",
   "metadata": {},
   "source": [
    "#### Custo versus iterações no Método SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba29895-06b9-49eb-9a4d-59a47bc59491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos do custo versus iterações\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(500 + np.arange(len(J_hist[500:])), J_hist[500:])\n",
    "ax1.set_title(\"Custo vs. iteração(primeiras iterações)\");  ax2.set_title(\"Custo vs. iteração(últimas iterações)\")\n",
    "ax1.set_ylabel('Custo')            ;  ax2.set_ylabel('Custo')\n",
    "ax1.set_xlabel('iteração')  ;  ax2.set_xlabel('iteração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260546c-a4a5-422c-80c7-f2c33c32c64e",
   "metadata": {},
   "source": [
    "#### Previsões Utilizando os parâmetros encontrados pelo Método SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad0d2a-5199-4233-a9c3-10c6213c2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Para uma corrente de 1 A temos uma tensão esperada de {w_final*1.0 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 2.35 A temos uma tensão esperada de {w_final*2.35 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 3 A temos uma tensão esperada de {w_final*3.0 + b_final:0.1f} Volts\")\n",
    "\n",
    "x_test = np.linspace(10, 20, 10)\n",
    "messages = [f\"Para uma corrente de {x:.2f} A temos uma tensão esperada de {w_final*x + b_final:0.1f} Volts\" for x in x_test]\n",
    "messages = \"\\n\".join(messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c35e7-2222-4e09-a72d-e402084ab6a4",
   "metadata": {},
   "source": [
    "#### Calculando a reta extrapolando os valores do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d131b-18cc-49bc-b8f0-ddfbc07f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wb = w_final * x_calc + b_final\n",
    "\n",
    "J = calcula_custo(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando a previsão feita pelo modelo\n",
    "plt.plot(x_calc, f_wb, c='b',label='Modelo do Método SGD')\n",
    "\n",
    "# Plotando os dados medidos\n",
    "plt.scatter(x_calc, y_calc, marker='x', c='r',label='Valores dos dados')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('Tensão (em Volts)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('Corrente (em Àmperes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Custo J para esse conjunto de parâmetros w,b: {J:.04f}\")\n",
    "print(f\"dj_dw: {dj_dw:.4e}, dj_db: {dj_db:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74351b-5fb8-4dc2-b3f5-36e27a7be3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores_para_w = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "valores_para_w = np.linspace(40, 60, 100, endpoint=True) # deixando a curva mais bonitinha\n",
    "\n",
    "qtdade_de_valores = valores_para_w.shape[0]\n",
    "\n",
    "J = np.zeros(qtdade_de_valores)\n",
    "\n",
    "for i in range(qtdade_de_valores):\n",
    "    w    = valores_para_w[i]\n",
    "    J[i] = calcula_custo(x_calc, y_calc, w, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando o custo em J para diferentes valores de w\n",
    "plt.plot(valores_para_w, J, c='b')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('J(w)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba7f53-8d1a-4783-9bc0-4f89503828bf",
   "metadata": {},
   "source": [
    "#### Rodando o Gradiente Conjugado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab79202-1f44-496a-8c6c-56d9fa359322",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteracoes = 3000\n",
    "# SGD trabalha melhor com taxas de aprendizado pequenas\n",
    "alpha = 0.1 # otimizado pelo método do olhômetro e VC (Vai Cavalo!!!)\n",
    "w_inicial = 0\n",
    "b_inicial = 0\n",
    "# Roda o método do gradiente\n",
    "w_final, b_final, J_hist, p_hist = gradiente_conjugado(x_trein ,y_trein, w_inicial, b_inicial, alpha,\n",
    "                                                    iteracoes, calcula_custo, calcula_gradiente)\n",
    "print(f\"(w,b) Encontrados pelo Método do Gradiente Conjugado: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682864e-a501-4c63-9acf-62ffee5ac42c",
   "metadata": {},
   "source": [
    "#### Custo versus iterações no Método Gradiente Conjugado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079650e-9c64-4f73-80f7-e7574a060cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos do custo versus iterações\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(15,6))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(500 + np.arange(len(J_hist[500:])), J_hist[500:])\n",
    "ax1.set_title(\"Custo vs. iteração(primeiras iterações)\");  ax2.set_title(\"Custo vs. iteração(últimas iterações)\")\n",
    "ax1.set_ylabel('Custo')            ;  ax2.set_ylabel('Custo')\n",
    "ax1.set_xlabel('iteração')  ;  ax2.set_xlabel('iteração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af9c5c-3920-417d-9683-6b9d1e478140",
   "metadata": {},
   "source": [
    "#### Previsões Utilizando os parâmetros encontrados pelo Método do Gradiente Conjugado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb51028-ffe1-4c9a-a510-d562c094847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Para uma corrente de 1 A temos uma tensão esperada de {w_final*1.0 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 2.35 A temos uma tensão esperada de {w_final*2.35 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 3 A temos uma tensão esperada de {w_final*3.0 + b_final:0.1f} Volts\")\n",
    "\n",
    "x_test = np.linspace(10, 20, 10)\n",
    "messages = [f\"Para uma corrente de {x:.2f} A temos uma tensão esperada de {w_final*x + b_final:0.1f} Volts\" for x in x_test]\n",
    "messages = \"\\n\".join(messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3178f-181b-418a-a956-f01b1f99fb54",
   "metadata": {},
   "source": [
    "#### Calculando a reta extrapolando os valores do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd82b7-32f3-408d-a377-1f0a8fad7ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wb = w_final * x_calc + b_final\n",
    "\n",
    "J = calcula_custo(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando a previsão feita pelo modelo\n",
    "plt.plot(x_calc, f_wb, c='b',label='Modelo do Método Gradiente Conjugado')\n",
    "\n",
    "# Plotando os dados medidos\n",
    "plt.scatter(x_calc, y_calc, marker='x', c='r',label='Valores dos dados')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('Tensão (em Volts)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('Corrente (em Àmperes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Custo J para esse conjunto de parâmetros w,b: {J:.04f}\")\n",
    "print(f\"dj_dw: {dj_dw:.4e}, dj_db: {dj_db:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ed785-d659-492c-be47-34378ec5e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores_para_w = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "valores_para_w = np.linspace(40, 60, 100, endpoint=True) # deixando a curva mais bonitinha\n",
    "\n",
    "qtdade_de_valores = valores_para_w.shape[0]\n",
    "\n",
    "J = np.zeros(qtdade_de_valores)\n",
    "\n",
    "for i in range(qtdade_de_valores):\n",
    "    w    = valores_para_w[i]\n",
    "    J[i] = calcula_custo(x_calc, y_calc, w, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando o custo em J para diferentes valores de w\n",
    "plt.plot(valores_para_w, J, c='b')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('J(w)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f48c05-e123-4c4d-816a-fc27fcad462e",
   "metadata": {},
   "source": [
    "#### Rodando o Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440136d-6db9-4e4e-a6dc-8624ae4b3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteracoes = 3000\n",
    "# SGD trabalha melhor com taxas de aprendizado pequenas\n",
    "w_inicial = 0\n",
    "b_inicial = 0\n",
    "# Roda o método do gradiente\n",
    "alpha = 0.9\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "w_final, b_final, J_hist, p_hist = adam(x_trein ,y_trein, w_inicial, b_inicial, alpha, beta1, beta2, epsilon, iteracoes, calcula_custo, calcula_gradiente)\n",
    "print(f\"(w,b) Encontrados pelo Método Adam: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160f2bc-e06c-4bb3-983f-26ad7b19faaa",
   "metadata": {},
   "source": [
    "### Custo versus iterações no Método Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d21a1-7690-45fd-90b1-52517769fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráficos do custo versus iterações\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(15,6))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(500 + np.arange(len(J_hist[500:])), J_hist[500:])\n",
    "ax1.set_title(\"Custo vs. iteração(primeiras iterações)\");  ax2.set_title(\"Custo vs. iteração(últimas iterações)\")\n",
    "ax1.set_ylabel('Custo')            ;  ax2.set_ylabel('Custo')\n",
    "ax1.set_xlabel('iteração')  ;  ax2.set_xlabel('iteração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b23cddb-89b1-43c5-acb1-e28254e5cf8d",
   "metadata": {},
   "source": [
    "#### Previsões Utilizando os parâmetros encontrados pelo Método Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e97af6-0421-41ce-9689-05bad81b1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Para uma corrente de 1 A temos uma tensão esperada de {w_final*1.0 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 2.35 A temos uma tensão esperada de {w_final*2.35 + b_final:0.1f} Volts\")\n",
    "print(f\"Para uma corrente de 3 A temos uma tensão esperada de {w_final*3.0 + b_final:0.1f} Volts\")\n",
    "\n",
    "x_test = np.linspace(10, 20, 10)\n",
    "messages = [f\"Para uma corrente de {x:.2f} A temos uma tensão esperada de {w_final*x + b_final:0.1f} Volts\" for x in x_test]\n",
    "messages = \"\\n\".join(messages)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b8e99-ae1d-4d24-9104-a60520294a26",
   "metadata": {},
   "source": [
    "#### Calculando a reta extrapolando os valores do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66efd1f-6115-4bef-b386-7d8f82a13997",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wb = w_final * x_calc + b_final\n",
    "\n",
    "J = calcula_custo(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "dj_dw, dj_db = calcula_gradiente(x_calc, y_calc, w_final, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando a previsão feita pelo modelo\n",
    "plt.plot(x_calc, f_wb, c='b',label='Modelo do Método Adam')\n",
    "\n",
    "# Plotando os dados medidos\n",
    "plt.scatter(x_calc, y_calc, marker='x', c='r',label='Valores dos dados')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('Tensão (em Volts)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('Corrente (em Àmperes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"Custo J para esse conjunto de parâmetros w,b: {J:.04f}\")\n",
    "print(f\"dj_dw: {dj_dw:.4e}, dj_db: {dj_db:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a858b-4b4b-47a6-9d2f-28f18e9a6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores_para_w = np.array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
    "valores_para_w = np.linspace(40, 60, 100, endpoint=True) # deixando a curva mais bonitinha\n",
    "\n",
    "qtdade_de_valores = valores_para_w.shape[0]\n",
    "\n",
    "J = np.zeros(qtdade_de_valores)\n",
    "\n",
    "for i in range(qtdade_de_valores):\n",
    "    w    = valores_para_w[i]\n",
    "    J[i] = calcula_custo(x_calc, y_calc, w, b_final)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plotando o custo em J para diferentes valores de w\n",
    "plt.plot(valores_para_w, J, c='b')\n",
    "\n",
    "# Ajustando o rótulo do eixo y\n",
    "plt.ylabel('J(w)')\n",
    "# Ajustando o rótulo do eixo x\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25d374-95b7-41b3-ab2e-a79f33d6b18e",
   "metadata": {},
   "source": [
    "### Conclusões\n",
    "\n",
    "Cada método, incluindo o apresentado na aula, tem seus prós e contras. Os métodos adicionais apresentam falhas: O RMSProp, Newton, SGD, Gradiente Conjugado e Adam, se mal ajustados, convergem para valores bem diferentes dos parâmetros utilizados para a construção do mapa inicial. Contudo, servem de parâmetro de desempenho e acurácia para o método do gradiente utilizado em aula.\n",
    "\n",
    "Destes algoritmos, percebe-se que:\n",
    "- Velocidade e convergência nem sempre andam juntas\n",
    "- Uma convergência próxima o suficiente muitas vezes é melhor que exatidão na resposta.\n",
    "- A escolha do algoritmo deve levar em conta a função do modelo."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
