{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f1cc65d6",
      "metadata": {
        "id": "f1cc65d6",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Fronteira de Decisão\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fe6af0",
      "metadata": {
        "id": "86fe6af0",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Atividade da Aula\n",
        "\n",
        "- **Aluno:** Augusto Mathias Adams\n",
        "\n",
        "#### Parte 1\n",
        "\n",
        "**Rode todo o código. Certifique-se que você o compreendeu.**\n",
        "\n",
        "Feito.\n",
        "\n",
        "#### Parte 2\n",
        "\n",
        "1. **Explique, com as suas próprias palavras, o conceito de Fronteira de Decisão no contexto da Regressão Logı́stica.**\n",
        "\n",
        "A fronteira de decisão, no contexto da regressão logística, é a linha ou hiperplano que separa as diferentes classes no espaço das variáveis  independentes. Ela corresponde ao ponto onde a função sigmoide retorna uma probabilidade de $0.5$, ou seja, o ponto em que o modelo não tem  referência para nenhuma das classes. \n",
        "\n",
        "2. **Considerando $w_0 = w_1 = 1$ e $b = −3$, calcule o valor do modelo $f_{\\vec{w}, b}(\\vec{x}^{\\left(i\\right)})$ para cada amostra de dados $\\vec{x}^{\\left(i\\right)}$ presente no código. O que esses valores representam? Os resultados estão coerentes com aquilo que é observado graficamente no código?**\n",
        "\n",
        "A seguinte tabela foi compilada na seção **Happy Hour** calculando a função sigmóide sobre os dados:\n",
        "\n",
        "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
        "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
        "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
        "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
        "| $y_{est}^{\\left(i\\right)}$ | 0.2689       | 0.2689       | 0.2689       | 0.6225       | 0.7311       | 0.6225       |\n",
        "\n",
        "Os valores de $y_{est}$ representam probabilidades de o ponto ter uma classificação positiva ($y=1$). \n",
        "\n",
        "Os 3 primeiros valores de $y$ têm classificação negativa ($y=0$) e probabilidade baixa de ter uma classificação positiva ($y_{est} < 0.5$). Da mesma forma, os 3 últimos valores de $y$ têm classificação positiva ($y=1$), sendo e probabilidade alta de classificação positiva ($y_{est} > 0.5$). Dado que todos os pontos têm seus valores de $y_{est}$ razoavelmente distantes da região de fronteira de decisão ($y_{est} = 0.5$), os resultados são coerentes com o que é apresentado no gráfico da fronteira de decisão mostrado ao longo deste *notebook*.\n",
        "\n",
        "3. **Calcule o valor da função perda para cada amostra $i$.**\n",
        "\n",
        "| $i$                        |    $1$ |    $2$ |    $3$ |    $4$ |    $5$ |    $6$ |\n",
        "|:---------------------------|-------:|-------:|-------:|-------:|-------:|-------:|\n",
        "| ${Perda}^{\\left(i\\right)}$ | 0.3133 | 0.3133 | 0.3133 | 0.4741 | 0.3133 | 0.4741 |\n",
        "\n",
        "4. **Calcule o custo $J(\\vec{w} , b)$**\n",
        "\n",
        "Custo calculado para $\\vec{w} = \\left[1, 1\\right]$ e $b = -3$ é igual a 0.3669\n",
        "\n",
        "5. **Estudo complementar**\n",
        "\n",
        "  - Otimização através dos algoritmos:\n",
        "    - Gradiente Descendente (Visto em Aula)\n",
        "    - RMSProp\n",
        "    - SGD (Gradiente Estocástico)\n",
        "    - Gradiente Conjugado\n",
        "    - Adam\n",
        "\n",
        "  - Método do Gradiente\n",
        "    - Iterações: 500\n",
        "    - Hiperparâmetros:\n",
        "      - $\\alpha = 2$\n",
        "    - Parâmetros otimizados\n",
        "      - $b = -14.24$ \n",
        "      - $\\vec{w} = \\left[5.28920767, 5.08615032\\right]$ \n",
        "    - Custo: 0.0170\n",
        "    - Conclusão preliminar: nada de braçada, braço curto\n",
        "    - Tabela de resultados:\n",
        "\n",
        "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
        "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
        "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
        "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
        "| $y_{est}^{\\left(i\\right)}$ | 0.0185       | 0.0205       | 0.0226       | 0.9847       | 0.9985       | 0.9772       |\n",
        "| ${Perda}^{\\left(i\\right)}$ | 10.2739      | 10.3754      | 10.4769      | 0.0000       | 0.0000       | 0.0000       |\n",
        "\n",
        "\n",
        "  - RMSProp\n",
        "    - Iterações: 100\n",
        "    - Hiperparâmetros:\n",
        "      - $\\alpha = 2$\n",
        "    - Parâmetros otimizados\n",
        "      - $b = -22.08$ \n",
        "      - $\\vec{w} = \\left[8.11392118, 7.92387849\\right]$\n",
        "    - Custo: 0.0020\n",
        "    - Conclusão preliminar: nada de braçada\n",
        "    - Tabela de resultados: \n",
        "\n",
        "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
        "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
        "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
        "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
        "| $y_{est}^{\\left(i\\right)}$ | 0.0022       | 0.0024       | 0.0026       | 0.9980       | 1.0000       | 0.9971       |\n",
        "| ${Perda}^{\\left(i\\right)}$ | 15.9428      | 16.0378      | 16.1328      | 0.0000       | 0.0000       | 0.0000       |\n",
        "\n",
        "  - SGD\n",
        "    - Iterações: 1000\n",
        "    - Hiperparâmetros:\n",
        "      - $\\alpha = 0.5$\n",
        "    - Parâmetros otimizados:\n",
        "      - $b = -18.39$ \n",
        "      - $\\vec{w} = \\left[6.79152059, 6.59181881\\right]$ \n",
        "    - Custo: 0.0055\n",
        "    - Conclusão preliminar: nada de braçada, braçadas curtas e lentas\n",
        "    - Tabela de resultados: \n",
        "\n",
        "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
        "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
        "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
        "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
        "| $y_{est}^{\\left(i\\right)}$ | 0.0060       | 0.0067       | 0.0074       | 0.9949       | 0.9998       | 0.9925       |\n",
        "| ${Perda}^{\\left(i\\right)}$ | 13.2835      | 13.3833      | 13.4832      | 0.0000       | 0.0000       | 0.0000       |\n",
        "\n",
        "\n",
        "  - Gradiente Conjugado\n",
        "\n",
        "    - Iterações: 200\n",
        "    - Hiperparâmetros\n",
        "      - $\\alpha = 5$\n",
        "    - Parâmetros otimizados: \n",
        "      - $b = -20.57$ \n",
        "      - $\\vec{w} = \\left[7.7605974,  7.21130688\\right]$ \n",
        "    - Custo: 0.0031\n",
        "    - Conclusão preliminar: nada de braçada\n",
        "    - Tabela de resultados: \n",
        "    \n",
        "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
        "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
        "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
        "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
        "| $y_{est}^{\\left(i\\right)}$ | 0.0028       | 0.0037       | 0.0049       | 0.9982       | 0.9999       | 0.9946       |\n",
        "| ${Perda}^{\\left(i\\right)}$ | 14.6973      | 14.9719      | 15.2465      | 0.0000       | 0.0000       | 0.0000       |\n",
        "\n",
        "  - Adam\n",
        "\n",
        "    - Iterações: 200\n",
        "\n",
        "    - Hiperparâmetros: \n",
        "\n",
        "      - $\\alpha = 1$\n",
        "\n",
        "      - $\\beta_1 = 0.9$\n",
        "\n",
        "      - $\\beta_2 = 0.999$\n",
        "\n",
        "      - $\\epsilon = 1 \\cdot 10^{-8}$\n",
        "\n",
        "    - Parâmetros otimizados: \n",
        "      - $b = -18.01$ \n",
        "      - $\\vec{w} = \\left[6.64355398, 6.44880118\\right]$     \n",
        "    - Custo: 0.0061\n",
        "    - Conclusão preliminar: nada de braçada\n",
        "    - Tabela de resultados: \n",
        "\n",
        "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
        "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
        "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
        "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
        "| $y_{est}^{\\left(i\\right)}$ | 0.0066       | 0.0073       | 0.0080       | 0.9942       | 0.9997       | 0.9915       |\n",
        "| ${Perda}^{\\left(i\\right)}$ | 12.9950      | 13.0924      | 13.1897      | 0.0000       | 0.0000       | 0.0000       |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad59113",
      "metadata": {},
      "source": [
        "## Objetivos\n",
        "\n",
        "Com este código, você irá:\n",
        "- Plotar a Fronteira de Decisão para um modelo de Regressão Logística. Isso lhe dará uma maior noção sobre o que o modelo está prevendo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "id": "6ee320a5",
      "metadata": {
        "id": "6ee320a5",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in /home/augusto/development/money_carousell/venv/lib/python3.10/site-packages (0.9.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tabulate\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7867bf8d",
      "metadata": {
        "id": "7867bf8d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Conjunto de dados\n",
        "\n",
        "Vamos supor que você possui o seguinte conjunto de dados\n",
        "- A variável de entrada `X` é uma numpy array com 6 exemplos de treinamento, cada um com duas características\n",
        "- A variável de saída `y` é também uma numpy array com 6 exemplos, sendo que `y` é sempre `0` ou `1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "id": "9baca38b",
      "metadata": {
        "id": "9baca38b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y = np.array([0, 0, 0, 1, 1, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182b8eb5",
      "metadata": {
        "id": "182b8eb5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Plotando os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "id": "89a44af6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "89a44af6",
        "outputId": "d6078197-4fa6-4981-a9c1-db47870e919c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAKtCAYAAADVdaSIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+M0lEQVR4nO3de3xddZ3v/3cKbQoDiYL2AgSEA1YQeqFCaZmRAkUEjofOOIioLajgiMUBGcdD/SkI6lSPNxxFQD1QAREELDrcbxYUCkJLx1KFEQttLU0rR0igQCjp/v2RaSRtk6bwTXaTPp+Px3742CvftfNJNtvwYq29dk2lUqkEAACAIgZUewAAAID+RGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABRU1ci68MILM3LkyNTV1aWuri7jx4/PzTff3On6mTNnpqampsNt8ODBvTgxAABA17au5jffZZdd8pWvfCV77bVXKpVKfvSjH+XYY4/Nww8/nLe//e0b3Keuri6PPfZY+/2ampreGhcAAGCjqhpZ73nPezrc//KXv5wLL7ww999/f6eRVVNTk2HDhvXGeAAAAJusqpH1aq2trbnmmmuyatWqjB8/vtN1zz//fHbbbbesWbMm+++/f/7t3/6t0yBbq6WlJS0tLe3316xZk7/85S/ZcccdHQkDAIAtWKVSyXPPPZeddtopAwaUeTdV1SNrwYIFGT9+fF566aVst912mTVrVvbZZ58Nrh0xYkQuueSSjBw5Mk1NTfn617+eCRMmZOHChdlll106/R4zZszIueee21M/AgAA0MctXbq0y6bYFDWVSqVS5JFeo5dffjlLlixJU1NTrr322vzwhz/M3Xff3Wlovdrq1auz995754QTTsgXv/jFTteteySrqakpu+66a5YuXZq6uroiPwcAAND3NDc3p6GhIc8++2zq6+uLPGbVj2QNGjQoe+65Z5Jk7NixefDBB/Ptb387F1988Ub3HThwYMaMGZPHH3+8y3W1tbWpra1db/vaqxoCAABbtpJvI9rsPidrzZo1HY46daW1tTULFizI8OHDe3gqAACA7qnqkazp06fnqKOOyq677prnnnsuV155ZWbPnp1bb701STJ16tTsvPPOmTFjRpLkvPPOy0EHHZQ999wzzz77bL72ta9l8eLFOfnkk6v5YwAAALSramStXLkyU6dOzfLly1NfX5+RI0fm1ltvzRFHHJEkWbJkSYcrfDzzzDM55ZRT0tjYmDe+8Y0ZO3Zs7rvvvm69fwsAAKA3VP3CF9XQ3Nyc+vr6NDU1eU8WAAB9Tmtra1avXl3tMfqEgQMHZquttur06z3RBlW/8AUAANA9lUoljY2NefbZZ6s9Sp/yhje8IcOGDeu1z8gVWQAA0EesDawhQ4Zk22237bVo6KsqlUpeeOGFrFy5Mkl67YJ5IgsAAPqA1tbW9sDacccdqz1On7HNNtskabsexJAhQ7o8dbCUze4S7gAAwPrWvgdr2223rfIkfc/a31lvvY9NZAEAQB/iFMFN19u/M5EFAABQkMgCAID+rLExWbq0e2uXLm1bz+sisgAAoL9qbEwOOyyZOHHjobV0adu6ww7brEPrZz/7Wd71rndlxx13TE1NTebPn1/tkdYjsgAAoL9avTppaUkWLeo6tNYG1qJFbes34w86XrVqVf72b/82X/3qV6s9SqdEFgAA9FcNDcns2ckee3QeWq8OrD32aFvf0FBshMsuuyw77rhjWlpaOmyfPHlypkyZssmPN2XKlJx99tmZNGlSqRGLE1kAANCfdRVaPRxYSXLccceltbU1v/jFL9q3rVy5MjfeeGM+8pGP5Fe/+lW22267Lm8//vGPi87U03wYMQAA9HdrQ2ttUE2cmFx+eTJlSo8GVtL2YcAf+MAHcumll+a4445LklxxxRXZddddM3HixLz00ksbfV/V0KFDi8/Vk0QWAABsCdYNrYMPbtveg4G11imnnJIDDjggy5Yty84775yZM2fmpJNOSk1NTbbZZpvsueeePfa9q8HpggAAsKVoaGg7gvVql1/eo4GVJGPGjMmoUaNy2WWXZe7cuVm4cGFOOumkJHG6IAAA0IctXdp2iuCrTZnS40eykuTkk0/O+eefn2XLlmXSpElp+O/v9453vMPpggAAQB+07kUuXv2erIkTezy0PvCBD+TTn/50fvCDH+Syyy5r376ppwv+5S9/yZIlS/LUU08lSR577LEkybBhwzJs2LCyQ79GThcEAID+bkNXEZwwYeOXdy+ovr4+733ve7Pddttl8uTJr/lxfvGLX2TMmDE55phjkiTvf//7M2bMmFx00UWFJn39RBYAAPRnXV2mvTufo1XQsmXL8sEPfjC1tbWv+TFOOumkVCqV9W5f+MIXyg36OoksAADor7rzOVi9EFrPPPNMZs2aldmzZ2fatGlFH3tz5D1ZAADQXw0cmNTWbvwy7a++vHttbdt+BY0ZMybPPPNMvvrVr2bEiBFFH3tzJLIAAKC/GjYsueuuZPXqjV/UYm1oDRzYtl9BTz75ZNHH29yJLAAA6M82JZh6+DLuWwrvyQIAAChIZAEAABQksgAAAAoSWQAAsAV4+eXkJz9Jjj462WuvZOedk7e9LTnuuOTmm5M1a6o9Yf/hwhcAANCPrVmTfPObyde+lqxcuf7XH3ssufbatqu8n3tu8qEP9f6M/Y3IAgCAfurll9ui6ZprNr520aJkypTkt79NvvrVpKam5+frr5wuCAAA/VClkpxySvcC69W+9rXk3/6tZ2YqoVKp5Oyzz87w4cOzzTbbZNKkSfnDH/5Q7bE6EFkAANAPXX99ctll628/5JDk+99PbropOf/8ZNSo9dd8/vNtR7Q2R//n//yf/Pu//3suuuiiPPDAA/mbv/mbHHnkkXnppZeqPVo7kQUAAP3Qt7/d8f622yY33pjMnt12hOuoo5LTT08efji5+OKOpwdWKsl3v1tmjssuuyw77rhjWlpaOmyfPHlypkyZskmPValUcv755+dzn/tcjj322IwcOTKXXXZZnnrqqVx//fVlBi5AZAEAQD+zcGFy990dt51/ftuVBddVU5N87GPJv/xLx+0//nHy7LOvf5bjjjsura2t+cUvftG+beXKlbnxxhvzkY98JL/61a+y3XbbdXn78Y9/nCR54okn0tjYmEmTJrU/Vn19fcaNG5c5c+a8/mELceELAADoZ/7jPzreHzIkOfHErvf51KeSb30raW1tu//CC8lddyX/8A+vb5ZtttkmH/jAB3LppZfmuOOOS5JcccUV2XXXXTNx4sS89NJLmT9/fpePMXTo0CRJY2Njh/uv/vrar20ORBYAAPQzy5d3vH/EEcmgQV3vs9NOyf77Jw8+2PnjvFannHJKDjjggCxbtiw777xzZs6cmZNOOik1NTXZZpttsueee5b5RpsJpwsCAEA/s/Zo1FrbbNO9/dZd98orZeYZM2ZMRo0alcsuuyxz587NwoULc9JJJyXJJp0uOGzYsCTJihUrOjz+ihUr2r+2OXAkCwAA+pkdd+x4/1e/aruYRVeffbVqVTJvXteP83qcfPLJOf/887Ns2bJMmjQpDQ0NSZJ3vOMd3T5dcPfdd8+wYcNy5513ZvTo0UmS5ubmPPDAAzn11FPLDfs6iSwAAOhnDjmk4/3HHmt7f9Xhh3e+z2WXJc8//9f7NTXJ3/1duZk+8IEP5NOf/nR+8IMf5LJXXVt+U04XrKmpyRlnnJEvfelL2WuvvbL77rvn85//fHbaaadMnjy53LCvk9MFAQCgn5k4MXnrWztuO+WUZOnSDa+fPz+ZPr3jtqOPTnbbrdxM9fX1ee9735vtttvudQXRZz7zmXzyk5/Mxz72sRxwwAF5/vnnc8stt2Tw4MHlhn2dRBYAAPQzAwYkn/hEx21PPJHst1/bBw0/9ljb6YEPP5x88pPJuHFJU1PH9dOmlZ9r2bJl+eAHP5ja2trX/Bg1NTU577zz0tjYmJdeeil33HFH3rpuUVaZ0wUBAKAf+qd/ajsF8NXvs2pqSr70pbZbV/7+75N3v7vcLM8880xmz56d2bNn53vf+165B95MiSwAAOiHBg9ObrwxOfTQ5NFHu7/fIYckV1zR9UUyNtWYMWPyzDPP5Ktf/WpGjBhR7oE3UyILAAD6qWHDknvvTU46af0PKF5XTU1y8snJd76TvI6z+TboySefLPuAmznvyQIAgH5shx2SX/wi+e1vk1NPTerrO3596NC2i14sWpR8//vlA2tL5EgWAABsAfbbL/ne99qOVD39dNvl2uvqkje9qeypgYgsAADoU9asWfO69t9qq7ajV//9+b5bhNf7O9tUIgsAAPqAQYMGZcCAAXnqqafy5je/OYMGDUqNQ1BdqlQqefnll/PnP/85AwYMyKBBg3rl+4osAADoAwYMGJDdd989y5cvz1NPPVXtcfqUbbfdNrvuumsGDOidS1KILAAA6CMGDRqUXXfdNa+88kpaW1urPU6fsNVWW2Xrrbfu1aN+IgsAAPqQmpqaDBw4MAMHDqz2KHTCJdwBAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBVY2sCy+8MCNHjkxdXV3q6uoyfvz43HzzzV3uc8011+Rtb3tbBg8enP322y833XRTL00LAACwcVWNrF122SVf+cpXMnfu3Dz00EM57LDDcuyxx2bhwoUbXH/fffflhBNOyEc/+tE8/PDDmTx5ciZPnpxHHnmklycHAADYsJpKpVKp9hCvtsMOO+RrX/taPvrRj673teOPPz6rVq3KDTfc0L7toIMOyujRo3PRRRd1+3s0Nzenvr4+TU1NqaurKzI3AADQ9/REG2w278lqbW3NVVddlVWrVmX8+PEbXDNnzpxMmjSpw7Yjjzwyc+bM6fKxW1pa0tzc3OEGAADQE6oeWQsWLMh2222X2trafPzjH8+sWbOyzz77bHBtY2Njhg4d2mHb0KFD09jY2OX3mDFjRurr69tvDQ0NxeYHAAB4tapH1ogRIzJ//vw88MADOfXUU3PiiSfmd7/7XdHvMX369DQ1NbXfli5dWvTxAQAA1tq62gMMGjQoe+65Z5Jk7NixefDBB/Ptb387F1988Xprhw0blhUrVnTYtmLFigwbNqzL71FbW5va2tpyQwMAAHSi6key1rVmzZq0tLRs8Gvjx4/PnXfe2WHb7bff3ul7uAAAAHpbVY9kTZ8+PUcddVR23XXXPPfcc7nyyisze/bs3HrrrUmSqVOnZuedd86MGTOSJKeffnoOOeSQfOMb38gxxxyTq666Kg899FC+//3vV/PHAAAAaFfVyFq5cmWmTp2a5cuXp76+PiNHjsytt96aI444IkmyZMmSDBjw14NtEyZMyJVXXpnPfe5z+exnP5u99tor119/ffbdd99q/QgAAAAdbHafk9UbfE4WAACQ9PPPyQIAAOgPRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgv6u8bGZOnS7q1durRtPQAAr5nIgv6ssTE57LBk4sSNh9bSpW3rDjtMaAEAvA4iC/qz1auTlpZk0aKuQ2ttYC1a1LZ+9erenBIAoF+pamTNmDEjBxxwQLbffvsMGTIkkydPzmOPPdblPjNnzkxNTU2H2+DBg3tpYuhjGhqS2bOTPfboPLReHVh77NG2vqGh92cFAOgnqhpZd999d6ZNm5b7778/t99+e1avXp13vetdWbVqVZf71dXVZfny5e23xYsX99LE0Ad1FVoCCwCguK2r+c1vueWWDvdnzpyZIUOGZO7cuXnnO9/Z6X41NTUZNmxYT48H/cfa0FobVBMnJpdfnkyZIrAAAArbrN6T1dTUlCTZYYcdulz3/PPPZ7fddktDQ0OOPfbYLFy4sMv1LS0taW5u7nCDLc66R7QOPlhgAQD0gM0mstasWZMzzjgjBx98cPbdd99O140YMSKXXHJJfv7zn+eKK67ImjVrMmHChPzpT3/qdJ8ZM2akvr6+/dbgXybZUjU0tB3BerXLLxdYAAAF1VQqlUq1h0iSU089NTfffHN+/etfZ5dddun2fqtXr87ee++dE044IV/84hc3uKalpSUtLS3t95ubm9PQ0JCmpqbU1dW97tmhz3j1e7DWciQLANiCNTc3p76+vmgbbBZHsk477bTccMMN+eUvf7lJgZUkAwcOzJgxY/L44493uqa2tjZ1dXUdbrDFWfciF/fe2/VVBwEAeE2qGlmVSiWnnXZaZs2albvuuiu77777Jj9Ga2trFixYkOHDh/fAhNBPbOgqghMmbPzy7gAAbLKqRta0adNyxRVX5Morr8z222+fxsbGNDY25sUXX2xfM3Xq1EyfPr39/nnnnZfbbrstixYtyrx58/KhD30oixcvzsknn1yNHwE2f11dpr07n6MFAMAmqWpkXXjhhWlqasrEiRMzfPjw9tvVV1/dvmbJkiVZvnx5+/1nnnkmp5xySvbee+8cffTRaW5uzn333Zd99tmnGj8CbN668zlYQgsAoKjN5sIXvakn3twGm6XGxuSww5KWlo1f3GJtkNXWJnfdlfgsOgBgC9ATbVDVDyMGetiwYW3BtHr1xq8euPaI1sCBAgsA4HUQWdDfbUowuYw7AMDrtllcwh0AAKC/EFkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIA6P8aG5OlS7u3dunStvUA8BqJLAD6t8bG5LDDkokTNx5aS5e2rTvsMKEFwGsmsgDo31avTlpakkWLug6ttYG1aFHb+tWre3NKAPqRqkbWjBkzcsABB2T77bfPkCFDMnny5Dz22GMb3e+aa67J2972tgwePDj77bdfbrrppl6YFoA+qaEhmT072WOPzkPr1YG1xx5t6xsaen9WAPqFqkbW3XffnWnTpuX+++/P7bffntWrV+dd73pXVq1a1ek+9913X0444YR89KMfzcMPP5zJkydn8uTJeeSRR3pxcgD6lK5CS2ABUFhNpVKpVHuItf785z9nyJAhufvuu/POd75zg2uOP/74rFq1KjfccEP7toMOOiijR4/ORRdd1K3v09zcnPr6+jQ1NaWurq7I7AD0AesG1eWXJ1OmCCyALVhPtMFm9Z6spqamJMkOO+zQ6Zo5c+Zk0qRJHbYdeeSRmTNnTqf7tLS0pLm5ucMNgC3Quke0Dj5YYAFQ3GYTWWvWrMkZZ5yRgw8+OPvuu2+n6xobGzN06NAO24YOHZrGLq4CNWPGjNTX17ffGvwRBdhyNTS0HcF6tcsvF1gAFLPZRNa0adPyyCOP5Kqrrir+2NOnT09TU1P7bWl3PysFgP5n6dK2UwRfbcqU7n+OFgBsxGYRWaeddlpuuOGG/PKXv8wuu+zS5dphw4ZlxYoVHbatWLEiw4YN63Sf2tra1NXVdbgBsAVa9z1Z997b9VUHAeA1qGpkVSqVnHbaaZk1a1buuuuu7L777hvdZ/z48bnzzjs7bLv99tszfvz4nhoTgP5gQ1cRnDBh45d3B4BNVNXImjZtWq644opceeWV2X777dPY2JjGxsa8+OKL7WumTp2a6dOnt98//fTTc8stt+Qb3/hGHn300XzhC1/IQw89lNNOO60aPwIAfUFXl2nvzudoAcAmqGpkXXjhhWlqasrEiRMzfPjw9tvVV1/dvmbJkiVZvnx5+/0JEybkyiuvzPe///2MGjUq1157ba6//vouL5YBwBasO5+DJbQAKGiz+pys3uJzsgC2II2NyWGHJS0tG79M+9ogq61N7ror6eL9vgD0Dz3RBlsXeRQA2FwNG9YWTKtXb/wy7WuPaA0cKLAAeM1EFgD936YEk8/LAuB12iwu4Q4AANBfiCwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFBQ0ch64IEHSj4cAABAn1M0so477riSDwcAANDnbL2pO7zvfe/b4PZKpZK//OUvr3sgAACAvmyTI+uOO+7I5Zdfnu22267D9kqlknvuuafYYAAAAH3RJkfWxIkTs/322+ed73znel8bOXJkkaEAAAD6qppKpVLpzsLnnnsu22+/fU/P0yuam5tTX1+fpqam1NXVVXscAACgSnqiDbp94Yu/+7u/S2NjY5FvCgAA0F91O7LGjBmTcePG5dFHH+2wff78+Tn66KOLDwYAANAXdTuyLr300px00kn527/92/z617/Of/3Xf+V973tfxo4dm6222qonZwQAAOgzNunCF+eee25qa2tzxBFHpLW1NYcffnjmzJmTAw88sKfmAwAA6FO6fSRrxYoVOf300/OlL30p++yzTwYOHJiTTjpJYAEAALxKtyNr9913zz333JNrrrkmc+fOzXXXXZePfexj+drXvtaT8wEAAPQp3T5d8JJLLsn73//+9vvvfve788tf/jL/83/+zzz55JO54IILemRAAACAvqTbR7JeHVhr7b///rnvvvty1113FR0KAACgr+p2ZHXmLW95S+67774SswAAAPR5rzuykuSNb3xjiYcBAADo84pEFgAAAG1EFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIK2rvYAbDn+9Kfk8suTRx5Jnn8++Zu/SfbaK/nQh9r+FwAA+gORRY+bNy/50peSX/wiaW1d/+vnnZe8613JZz+bHHJI788HAAAlOV2QHnX55cm4ccmsWRsOrLVuuy2ZODH5+td7bTQAAOgRIosec/XVydSpySuvdH+ff/3X5Pzze2wkAADocU4XpEcsXpycdNL62//mb5L3vrftPVhLliTXXps880zHNWeemfzd3yVjx/bKqAAAUFTVj2Tdc889ec973pOddtopNTU1uf7667tcP3v27NTU1Kx3a2xs7J2B6Zbvfjd56aWO2049NXnqqeRHP0o+97nk+99Pli1Lzj6747pKJfnmN3tvVgAAKKnqkbVq1aqMGjUqF1xwwSbt99hjj2X58uXttyFDhvTQhGyqF19MLrmk47YPfCC54IKkrq7j9m22Sc49N/n0pztuv+aaZOXKnp0TAAB6QtVPFzzqqKNy1FFHbfJ+Q4YMyRve8IbyA/G63X578pe/dNz2hS8kNTWd73PWWR2Pfq1enVx3XdvRLwAA6EuqfiTrtRo9enSGDx+eI444Ivfee2+Xa1taWtLc3NzhRs9ZvLjj/f333/jnYO24YzJpUsdtS5aUnQsAAHpDn4us4cOH56KLLsp1112X6667Lg0NDZk4cWLmzZvX6T4zZsxIfX19+62hoaEXJ97yvPhix/vdPZNz3XXrPg4AAPQFVT9dcFONGDEiI0aMaL8/YcKE/PGPf8y3vvWtXH755RvcZ/r06TnzzDPb7zc3NwutHlRf3/H+ggXJmjXJgI0k/X/+Z9ePAwAAfUGfO5K1IQceeGAef/zxTr9eW1uburq6Djd6zrqXXl+2LLn55q73efjhZO7cjtv237/sXAAA0Bv6RWTNnz8/w4cPr/YY/LexY5NRozpu+9Sn1r8YxlovvJB84hMdt+20U3L00T0zHwAA9KSqR9bzzz+f+fPnZ/78+UmSJ554IvPnz8+S/77qwfTp0zN16tT29eeff35+/vOf5/HHH88jjzySM844I3fddVemTZtWjfHZgJqaZN2n4w9/aIuvyy776xUEX3kl+dnPknHjkvvv77j+lFOSgQN7Z14AACip6u/Jeuihh3LooYe231/73qkTTzwxM2fOzPLly9uDK0lefvnl/Mu//EuWLVuWbbfdNiNHjswdd9zR4TGovilTkm9/O1m48K/bnnwyOfHE5OMfT3bbre00wueeW3/fnXdO/vmfe21UAAAoqqZSqVSqPURva25uTn19fZqamrw/qwctXpwcfHBbTHXXG96Q3H13MnJkj40FAADteqINqn66IP3Xbrslc+Z0/wIWe+6Z3HuvwAIAoG8TWfSohobkgQeS665LDj98w2sOOqjtvVoLFiT77NO78wEAQGlVf08W/d/WWyf/8A9tt8WLk9/9ru29WNtt13b06q1vrfaEAABQjsiiV+22W9sNAAD6K6cLAgAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIK2rvYAQO/5r/9Krrgi+cMfkhdeSLbfPtl332TKlGTnnas9HQBA/yCyYAtw113JjBnJHXds+Ouf+1zy93+f/H//XzJ6dK+OBgDQ7zhdEPqxSiX5yleSww/vPLCSpLU1ufbaZNy45Cc/6b35AAD6I5EF/djXv55Mn9799S+/nHzwg8nPftZzMwEA9HdOF4R+6v77k898Zv3tO+yQ/OM/Jg0NyWOPtQXVCy/89euVStt7tA48MNlll96bFwCgv6j6kax77rkn73nPe7LTTjulpqYm119//Ub3mT17dvbff//U1tZmzz33zMyZM3t8TuhrvvGN9bedd16ybFly8cVt78O6/PK2+x/7WMd1L7yQXHhh78wJANDfVD2yVq1alVGjRuWCCy7o1vonnngixxxzTA499NDMnz8/Z5xxRk4++eTceuutPTwp9B1PPZXMmtVx22c/m3z+88ngwR23v+ENyUUXtR3derUf/jBpaenRMQEA+qWaSqVSqfYQa9XU1GTWrFmZPHlyp2v+9//+37nxxhvzyCOPtG97//vfn2effTa33HJLt75Pc3Nz6uvr09TUlLq6utc7Nmx2/v3fk9NP/+v9bbdtC6/6+s73+d3vkre/veO2m25KjjqqZ2YEANgc9EQbVP1I1qaaM2dOJk2a1GHbkUcemTlz5nS6T0tLS5qbmzvcoD9bvLjj/Xe/u+vASpJ99mn7zKxXW7Kk7FwAAFuCPhdZjY2NGTp0aIdtQ4cOTXNzc1588cUN7jNjxozU19e33xoaGnpjVKiadV8KQ4Z0b79113XykgIAoAt9LrJei+nTp6epqan9tnTp0mqPBD1q3aNW//mfG9/nlVeSV52Fu8HHAQBg4/rcJdyHDRuWFStWdNi2YsWK1NXVZZttttngPrW1tamtre2N8WCzMHZsx/tz5iQLF67/nqtX+4//SFau7Lht//3LzwYA0N/1uSNZ48ePz5133tlh2+23357x48dXaSLY/LznPcmb39xx28c/3vnpf08/nXz60x23HXBAMmpUz8wHANCfVT2ynn/++cyfPz/z589P0naJ9vnz52fJf7/jfvr06Zk6dWr7+o9//ONZtGhRPvOZz+TRRx/N9773vfz0pz/Npz71qWqMD5ul2trklFM6bvv1r5Px45Prr287NTBpi66ZM9uOfC1a1HH9Jz7RG5MCAPQ/Vb+E++zZs3PooYeut/3EE0/MzJkzc9JJJ+XJJ5/M7NmzO+zzqU99Kr/73e+yyy675POf/3xOOumkbn9Pl3BnS/D008l++yWNjet/ra4u2WmntqsQbujo1qhRyW9+kwwa1PNzAgBUU0+0QdUjqxpEFluKefOSQw9NNuVTC3bdNbn33mSXXXpuLgCAzYXPyQI2yf77t50muPvu3Vt/wAFtF8kQWAAAr53Ign5uv/2S3/2u7b1XBx644TVHHNH2Xq05c9pOIwQA4LVzuqDTBdnCPPZY8vjjyapVbe/N2meftlMEAQC2RD3RBn3uc7KA12fEiLYbAAA9w+mCAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoKCtqz0AwJZk3rzkqquSxYuTl15K3vCG5B3vSD74wWSHHao9HQBQQk2lUqlUe4je1tzcnPr6+jQ1NaWurq7a4wD9XKWSXHdd8vWvJw88sOE1gwcnJ5yQfO5zyR579O58ALAl64k2cLogQA9qbU0++cnkuOM6D6yk7ajWpZcmY8Ykd93Ve/MBW5DGxmTp0u6tXbq0bT3wmogsgB5SqST//M/JBRd0f5/m5uSYY5I5c3puLmAL1NiYHHZYMnHixkNr6dK2dYcdJrTgNRJZAD3kmmuS731v/e277NIWX1/8YvLe9yYDB3b8+ksvtW1ftap35gS2AKtXJy0tyaJFXYfW2sBatKht/erVvTkl9BubRWRdcMEFectb3pLBgwdn3Lhx+c1vftPp2pkzZ6ampqbDbfDgwb04LUD3fO1rHe8PGpRcdFHyxBPJt7/d9v6ra69tuwjG//pfHdcuX55ceWXvzQr0cw0NyezZbW/67Cy0Xh1Ye+zRtr6hofdnhX6g6pF19dVX58wzz8w555yTefPmZdSoUTnyyCOzcuXKTvepq6vL8uXL22+LFy/uxYkBNu7BB5OHHuq47bvfTf7pn5Kt17mu6/DhbRfGOOigjtu/9722Uw4BiugqtAQWFFX1yPrmN7+ZU045JR/+8Iezzz775KKLLsq2226bSy65pNN9ampqMmzYsPbb0KFDe3FigI37yU863t911+QjH+l8/dZbJ2ef3XHb/PnJo48WHw3Ykm0otO67T2BBYVWNrJdffjlz587NpEmT2rcNGDAgkyZNypwu3vX9/PPPZ7fddktDQ0OOPfbYLFy4sMvv09LSkubm5g43gJ607gH2f/zHZKutut7nyCOTda8cu2RJ2bkA1gutgw8WWFBYVSPr6aefTmtr63pHooYOHZrGTq5mM2LEiFxyySX5+c9/niuuuCJr1qzJhAkT8qc//anT7zNjxozU19e33xr8nwfQw158seP9IUM2vs+AAcmb3tT14wAU0dCQXH55x22XXy6woJCqny64qcaPH5+pU6dm9OjROeSQQ/Kzn/0sb37zm3PxxRd3us/06dPT1NTUflva3c+IAHiN6us73v/P/9z4Ps8+u/4RsHUfB6CIpUuTKVM6bpsypfufowV0qaqR9aY3vSlbbbVVVqxY0WH7ihUrMmzYsG49xsCBAzNmzJg8/vjjna6pra1NXV1dhxtATxo7tuP9WbOSp5/uep9LL2378OK1tt462W+/8rMBW7h1L3Jx771dX3UQ2GRVjaxBgwZl7NixufPOO9u3rVmzJnfeeWfGjx/frcdobW3NggULMnz48J4aE2CTTZnS8fOvXnopmTatY0S92uOPt31u1qv9/d+vf/ogwOuyoasITpiw8cu7A5uk6qcLnnnmmfnBD36QH/3oR/n973+fU089NatWrcqHP/zhJMnUqVMzffr09vXnnXdebrvttixatCjz5s3Lhz70oSxevDgnn3xytX4EgPUMHZocd1zHbT/9aXL44cldd/310uxNTcm//3syblzyzDMd13/iE70zK7CF6Ooy7d35HC2g27be+JKedfzxx+fPf/5zzj777DQ2Nmb06NG55ZZb2i+GsWTJkgwY8NcWfOaZZ3LKKaeksbExb3zjGzN27Njcd9992Weffar1IwBs0NlnJz//ebJq1V+33X13W2jtuGPbUapFi5LVq9ff9+ijk0MO6b1ZgX6uO5+DtTa01q6bONHVBuE1qqlUtryPumxubk59fX2ampq8PwvoUTffnBx77IZDqjOjR7fFmP97AoppbEwOOyxpadl4OK0NstratkPv3XyfPPRVPdEGIsu/xQA9bPbs5H3vS/78542vPeqotg8ydlVBoLjGxrb/4tOdI1NLl7a9sVRgsQXoiTao+nuyAPq7iRPbLmzxne8ke++9/te32ip573uTO+5IbrxRYAE9ZNiw7p/619AgsOB1cCTLkSygF1UqyW9/myxZ0nbFwfr6ttMDu/NhxQBAeT3RBlW/8AXAlqSmJhk1qu0GAPRPThcEAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAUJLIAAAAKElkAAAAFiSwAAICCRBYAAEBBIgsAAKAgkQUAAFCQyAIAAChIZAEAABQksgAAAAoSWQAAAAWJLAAAgIJEFgAAQEEiCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoaLOIrAsuuCBvectbMnjw4IwbNy6/+c1vulx/zTXX5G1ve1sGDx6c/fbbLzfddFMvTQoAANC1qkfW1VdfnTPPPDPnnHNO5s2bl1GjRuXII4/MypUrN7j+vvvuywknnJCPfvSjefjhhzN58uRMnjw5jzzySC9PDgAAsL6aSqVSqeYA48aNywEHHJDvfve7SZI1a9akoaEhn/zkJ3PWWWett/7444/PqlWrcsMNN7RvO+iggzJ69OhcdNFF3fqezc3Nqa+vT1NTU+rq6sr8IAAAQJ/TE22wdZFHeY1efvnlzJ07N9OnT2/fNmDAgEyaNClz5szZ4D5z5szJmWee2WHbkUcemeuvv77T79PS0pKWlpb2+01NTUnafqEAAMCWa20TlDz2VNXIevrpp9Pa2pqhQ4d22D506NA8+uijG9ynsbFxg+sbGxs7/T4zZszIueeeu972hoaG1zA1AADQ3/y///f/Ul9fX+SxqhpZvWX69Okdjn49++yz2W233bJkyZJiv0g2TXNzcxoaGrJ06VKnbFaB33/1eQ6qz3NQfZ6D6vMcVJ/noPqampqy6667Zocddij2mFWNrDe96U3ZaqutsmLFig7bV6xYkWHDhm1wn2HDhm3S+iSpra1NbW3tetvr6+v9w1xldXV1noMq8vuvPs9B9XkOqs9zUH2eg+rzHFTfgAHlrglY1asLDho0KGPHjs2dd97Zvm3NmjW58847M378+A3uM378+A7rk+T222/vdD0AAEBvqvrpgmeeeWZOPPHEvOMd78iBBx6Y888/P6tWrcqHP/zhJMnUqVOz8847Z8aMGUmS008/PYcccki+8Y1v5JhjjslVV12Vhx56KN///ver+WMAAAAk2Qwi6/jjj8+f//znnH322WlsbMzo0aNzyy23tF/cYsmSJR0O3U2YMCFXXnllPve5z+Wzn/1s9tprr1x//fXZd999u/09a2trc84552zwFEJ6h+eguvz+q89zUH2eg+rzHFSf56D6PAfV1xPPQdU/JwsAAKA/qep7sgAAAPobkQUAAFCQyAIAAChIZAEAABTUbyPrggsuyFve8pYMHjw448aNy29+85su119zzTV529velsGDB2e//fbLTTfd1EuT9k+b8vufOXNmampqOtwGDx7ci9P2P/fcc0/e8573ZKeddkpNTU2uv/76je4ze/bs7L///qmtrc2ee+6ZmTNn9vic/dmmPgezZ89e73VQU1OTxsbG3hm4n5kxY0YOOOCAbL/99hkyZEgmT56cxx57bKP7+VtQzmt5Dvw9KOvCCy/MyJEj2z/kdvz48bn55pu73MdroKxNfQ68BnrWV77yldTU1OSMM87ocl2J10G/jKyrr746Z555Zs4555zMmzcvo0aNypFHHpmVK1ducP19992XE044IR/96Efz8MMPZ/LkyZk8eXIeeeSRXp68f9jU33/S9inny5cvb78tXry4Fyfuf1atWpVRo0blggsu6Nb6J554Isccc0wOPfTQzJ8/P2eccUZOPvnk3HrrrT08af+1qc/BWo899liH18KQIUN6aML+7e677860adNy//335/bbb8/q1avzrne9K6tWrep0H38Lynotz0Hi70FJu+yyS77yla9k7ty5eeihh3LYYYfl2GOPzcKFCze43mugvE19DhKvgZ7y4IMP5uKLL87IkSO7XFfsdVDphw488MDKtGnT2u+3trZWdtppp8qMGTM2uP5973tf5Zhjjumwbdy4cZV/+qd/6tE5+6tN/f1feumllfr6+l6absuTpDJr1qwu13zmM5+pvP3tb++w7fjjj68ceeSRPTjZlqM7z8Evf/nLSpLKM8880yszbWlWrlxZSVK5++67O13jb0HP6s5z4O9Bz3vjG99Y+eEPf7jBr3kN9I6ungOvgZ7x3HPPVfbaa6/K7bffXjnkkEMqp59+eqdrS70O+t2RrJdffjlz587NpEmT2rcNGDAgkyZNypw5cza4z5w5czqsT5Ijjzyy0/V07rX8/pPk+eefz2677ZaGhoaN/hceyvMa2HyMHj06w4cPzxFHHJF777232uP0G01NTUmSHXbYodM1Xgc9qzvPQeLvQU9pbW3NVVddlVWrVmX8+PEbXOM10LO68xwkXgM9Ydq0aTnmmGPW++d7Q0q9DvpdZD399NNpbW3N0KFDO2wfOnRop+9taGxs3KT1dO61/P5HjBiRSy65JD//+c9zxRVXZM2aNZkwYUL+9Kc/9cbIpPPXQHNzc1588cUqTbVlGT58eC666KJcd911ue6669LQ0JCJEydm3rx51R6tz1uzZk3OOOOMHHzwwdl33307XedvQc/p7nPg70F5CxYsyHbbbZfa2tp8/OMfz6xZs7LPPvtscK3XQM/YlOfAa6C8q666KvPmzcuMGTO6tb7U62DrTVoNPWD8+PEd/ovOhAkTsvfee+fiiy/OF7/4xSpOBr1nxIgRGTFiRPv9CRMm5I9//GO+9a1v5fLLL6/iZH3ftGnT8sgjj+TXv/51tUfZYnX3OfD3oLwRI0Zk/vz5aWpqyrXXXpsTTzwxd999d6f/kk95m/IceA2UtXTp0px++um5/fbbe/0CIv0ust70pjdlq622yooVKzpsX7FiRYYNG7bBfYYNG7ZJ6+nca/n9r2vgwIEZM2ZMHn/88Z4YkQ3o7DVQV1eXbbbZpkpTceCBBwqD1+m0007LDTfckHvuuSe77LJLl2v9LegZm/IcrMvfg9dv0KBB2XPPPZMkY8eOzYMPPphvf/vbufjii9db6zXQMzblOViX18DrM3fu3KxcuTL7779/+7bW1tbcc889+e53v5uWlpZstdVWHfYp9Trod6cLDho0KGPHjs2dd97Zvm3NmjW58847Oz3/dfz48R3WJ8ntt9/e5fmybNhr+f2vq7W1NQsWLMjw4cN7akzW4TWweZo/f77XwWtUqVRy2mmnZdasWbnrrruy++67b3Qfr4OyXstzsC5/D8pbs2ZNWlpaNvg1r4He0dVzsC6vgdfn8MMPz4IFCzJ//vz22zve8Y588IMfzPz589cLrKTg62DTr8+x+bvqqqsqtbW1lZkzZ1Z+97vfVT72sY9V3vCGN1QaGxsrlUqlMmXKlMpZZ53Vvv7ee++tbL311pWvf/3rld///veVc845pzJw4MDKggULqvUj9Gmb+vs/99xzK7feemvlj3/8Y2Xu3LmV97///ZXBgwdXFi5cWK0foc977rnnKg8//HDl4YcfriSpfPOb36w8/PDDlcWLF1cqlUrlrLPOqkyZMqV9/aJFiyrbbrtt5V//9V8rv//97ysXXHBBZauttqrccsst1foR+rxNfQ6+9a1vVa6//vrKH/7wh8qCBQsqp59+emXAgAGVO+64o1o/Qp926qmnVurr6yuzZ8+uLF++vP32wgsvtK/xt6BnvZbnwN+Dss4666zK3XffXXniiScqv/3tbytnnXVWpaampnLbbbdVKhWvgd6wqc+B10DPW/fqgj31OuiXkVWpVCrf+c53Krvuumtl0KBBlQMPPLBy//33t3/tkEMOqZx44okd1v/0pz+tvPWtb60MGjSo8va3v71y44039vLE/cum/P7POOOM9rVDhw6tHH300ZV58+ZVYer+Y+3lwNe9rf29n3jiiZVDDjlkvX1Gjx5dGTRoUGWPPfaoXHrppb0+d3+yqc/BV7/61cr/+B//ozJ48ODKDjvsUJk4cWLlrrvuqs7w/cCGfvdJOvxz7W9Bz3otz4G/B2V95CMfqey2226VQYMGVd785jdXDj/88PZ/ua9UvAZ6w6Y+B14DPW/dyOqp10FNpVKpbNqxLwAAADrT796TBQAAUE0iCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILgH7vJz/5SbbZZpssX768fduHP/zhjBw5Mk1NTVWcDID+qKZSqVSqPQQA9KRKpZLRo0fnne98Z77zne/knHPOySWXXJL7778/O++8c7XHA6CfcSQLgH6vpqYmX/7yl/ODH/wgX/7yl/Od73wnt9xyS3tg3XDDDRkxYkT22muv/PCHP6zytAD0dY5kAbDF2H///bNw4cLcdtttOeSQQ5Ikr7zySvbZZ5/88pe/TH19fcaOHZv77rsvO+64Y5WnBaCvciQLgC3CLbfckkcffTStra0ZOnRo+/bf/OY3efvb356dd9452223XY466qjcdtttVZwUgL5OZAHQ782bNy/ve9/78n//7//N4Ycfns9//vPtX3vqqac6vC9r5513zrJly6oxJgD9xNbVHgAAetKTTz6ZY445Jp/97GdzwgknZI899sj48eMzb9687L///tUeD4B+yJEsAPqtv/zlL3n3u9+dY489NmeddVaSZNy4cTnqqKPy2c9+Nkmy0047dThytWzZsuy0005VmReA/sGFLwDYor3yyivZe++9M3v2bBe+AKAIpwsCsEXbeuut841vfCOHHnpo1qxZk8985jMCC4DXxZEsAACAgrwnCwAAoCCRBQAAUJDIAgAAKEhkAQAAFCSyAAAAChJZAAAABYksAACAgkQWAABAQSILAACgIJEFAABQkMgCAAAoSGQBAAAU9P8Doc0XVyIk7koAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pos = y == 1 # identifica os elementos em y que são iguais a 1\n",
        "neg = y == 0 # identifica os elementos em y que são iguais a 0\n",
        "\n",
        "# tô quase cego irmão, num faz figura pequena!!! hahahahaha\n",
        "fig,ax = plt.subplots(1,1,figsize=(10,8))\n",
        "ax.scatter(X[pos,0], X[pos,1], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "ax.scatter(X[neg,0], X[neg,1], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "              edgecolors='b',lw=3)\n",
        "\n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.set_xlabel('$x_0$')\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b5bea62",
      "metadata": {
        "id": "7b5bea62",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "  \n",
        "## Plotando a Fronteira de Decisão para o modelo com $w_0=1$, $w_1=1$ e $b=-3$\n",
        "\n",
        "\n",
        "Já vimos que o modelo de Regressão Logística aplica a função sigmoide para o modelo de regressão linear, conforme mostrado abaixo:\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b )$$\n",
        "\n",
        "  onde\n",
        "\n",
        "  $g(z) = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "Ou seja, se considerarmos $w_1=1$, $w_2=1$ e $b=-3$, significa que nosso modelo é\n",
        "\n",
        "$$ f(\\mathbf{x}) = g(-3 + x_0+x_1) $$\n",
        "\n",
        "\n",
        "* A partir daquilo que aprendemos, sabemos que o modelo irá prever $y=1$ quando $-3 + x_0+x_1 >= 0$\n",
        "\n",
        "Vamos verificar o que significa isso graficamente. Faremos isso plotando a Fronteira de Decisão $-3 + x_0+x_1 = 0$, que equivale a $x_1 = 3 - x_0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "id": "f3c4a990",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "f3c4a990",
        "outputId": "64578fe6-3445-41f0-a310-11aa65a07a89",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Escolhendo valores entre 0 e 6\n",
        "x0 = np.arange(0,6)\n",
        "\n",
        "x1 = 3 - x0\n",
        "\n",
        "# tô quase cego irmão, num faz figura pequena!!! hahahahaha\n",
        "fig,ax = plt.subplots(1,1,figsize=(10,8))\n",
        "\n",
        "# Plotando a Fronteira de Decisão\n",
        "ax.plot(x0,x1, c=\"b\")\n",
        "ax.axis([0, 4, 0, 3.5]) # limitando o display do gráfico\n",
        "\n",
        "# Preenche a região abaixo da linha\n",
        "ax.fill_between(x0,x1, alpha=0.2)\n",
        "\n",
        "# Plotando os dados também:\n",
        "ax.scatter(X[pos,0], X[pos,1], marker='x', s=80, c = 'red', label=\"y=1\")\n",
        "ax.scatter(X[neg,0], X[neg,1], marker='o', s=100, label=\"y=0\", facecolors='none',\n",
        "              edgecolors='b',lw=3)\n",
        "\n",
        "ax.set_ylabel('$x_1$')\n",
        "ax.set_xlabel('$x_0$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb5b861",
      "metadata": {
        "id": "dbb5b861",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "* No gráfico acima, a linha azul representa a linha $x_0 + x_1 - 3 = 0$ e ela intercepta o eixo x0 em 3 (para $x_1=0$, tem-se $x_0=3$)\n",
        "\n",
        "\n",
        "* A região com cor azul representa $-3 + x_0+x_1 < 0$. A região acima da linha representa $-3 + x_0+x_1 > 0$.\n",
        "\n",
        "\n",
        "* Qualquer ponto na região com cor azul (abaixo da linha) é classificado como $y=0$. Qualquer ponto acima da curva é classificado como $y=1$. Essa linha é chamada de \"Fronteira de Decisão\".\n",
        "\n",
        "Também vimos que, usando termos polinomiais de maior ordem (por exemplo, $f(x) = g( x_0^2 + x_1 -1)$ ), Fronteiras de Decisão mais complexas podem ser obtidas!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfde5655",
      "metadata": {
        "id": "cfde5655",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Parabéns!\n",
        "\n",
        "Você explorou a conceito de Fronteira de Decisão no contexto de Regressão Logística."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb6f55f8",
      "metadata": {},
      "source": [
        "### Happy Hour"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8018cd6c",
      "metadata": {},
      "source": [
        "##### Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "id": "3a9c8862",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoide(z):\n",
        "    \"\"\"\n",
        "    Calcula o valor sigmoide de z\n",
        "\n",
        "    Argumento:\n",
        "        z (ndarray): Um escalar ou numpy array de qualquer tamanho.\n",
        "\n",
        "    Retorna:\n",
        "        g (ndarray): sigmoid(z), com o mesmo shape de z\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "    return g\n",
        "\n",
        "def calcula_perda(x, y, w, b):\n",
        "    \"\"\"\n",
        "        Função perda\n",
        "        Args:\n",
        "            x (ndarray): valores da variável independente\n",
        "            y (ndarray): valores da variável independente\n",
        "            w (float): valor do coeficiente angular\n",
        "            b (float): valor do intercepto\n",
        "        Returns:\n",
        "            perdas (ndarray): as perdas associadas aos parâmetros w e b em cada ponto\n",
        "    \"\"\"\n",
        "\n",
        "    z = np.dot(x, w) + b\n",
        "    y_est = sigmoide(z) \n",
        "    perdas = - (y * np.log(y_est) + (1 - y) * np.log(1 - y_est))\n",
        "    return perdas\n",
        "\n",
        "\n",
        "def calcula_custo(x, y, w, b):\n",
        "    \"\"\"\n",
        "        Calcula o custo em função da perda\n",
        "        Args:\n",
        "            x (ndarray): valores da variável independente\n",
        "            y (ndarray): valores da variável independente\n",
        "            w (ndarray): valor do coeficiente angular\n",
        "            b (float): valor do intercepto\n",
        "        Returns:\n",
        "            perdas (ndarray): as perdas associadas aos parâmetros w e b em cada ponto\n",
        "    \"\"\"\n",
        "    n = len(y) \n",
        "    perdas_totais = np.sum(calcula_perda(x, y, w, b))\n",
        "    \n",
        "    return perdas_totais / n\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6033cdc4",
      "metadata": {},
      "source": [
        "#### Questão 2 da parte 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "id": "e68239ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
            "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
            "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
            "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
            "| $y_{est}^{\\left(i\\right)}$ | 0.2689       | 0.2689       | 0.2689       | 0.6225       | 0.7311       | 0.6225       |\n"
          ]
        }
      ],
      "source": [
        "# parâmetros indicados\n",
        "\n",
        "w = np.array([1, 1])\n",
        "b = -3\n",
        "z = np.dot(X, w) + b\n",
        "\n",
        "# calculando a função sigmóide para cada ponto\n",
        "y_pred = sigmoide(z)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\")\n",
        "xc = [\"({:.2f}, {:.2f})\".format(x[0], x[1]) for x in X]\n",
        "yc = [\"{:.4f}\".format(x) for x in y]\n",
        "yc.insert(0, \"$y^{\\left(i\\\\right)}$\") \n",
        "xc.insert(0,\"$X^{\\left(i\\\\right)}$\")\n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"$y_{est}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(xc)\n",
        "tabela.append(yc)\n",
        "tabela.append(y_predc)\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35f2fb1",
      "metadata": {},
      "source": [
        "#### Questão 3 da parte 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "id": "3a9a3f9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| $i$                        |    $1$ |    $2$ |    $3$ |    $4$ |    $5$ |    $6$ |\n",
            "|:---------------------------|-------:|-------:|-------:|-------:|-------:|-------:|\n",
            "| ${Perda}^{\\left(i\\right)}$ | 0.3133 | 0.3133 | 0.3133 | 0.4741 | 0.3133 | 0.4741 |\n"
          ]
        }
      ],
      "source": [
        "# calculando a função de perda para cada ponto\n",
        "y_pred = calcula_perda(X, y, w, b)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\") \n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"${Perda}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(y_predc)\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b3cfcc",
      "metadata": {},
      "source": [
        "#### Questão 4 da parte 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "id": "e4653745",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3669\n"
          ]
        }
      ],
      "source": [
        "# calculando a função sigmóide para cada ponto\n",
        "custo = calcula_custo(X, y, w, b)\n",
        "print(f\"{custo:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260991d7",
      "metadata": {},
      "source": [
        "#### Me adiantando no conteúdo das próximas aulas......"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4db0472",
      "metadata": {},
      "source": [
        "##### Funções Auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "id": "d44f85ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def calcula_gradiente(x, y, w, b):\n",
        "    \"\"\"\n",
        "    Calcula Gradiente para Regressão Linear\n",
        "    Args:\n",
        "      x(ndarray): Dados, contendo m exemplos com n características\n",
        "      y (ndarray) : valores alvo\n",
        "      w (ndarray) : parâmetros w do modelo\n",
        "      b (float) : parâmetro b do modelo\n",
        "\n",
        "    Retorna:\n",
        "      dj_dw (float): O gradiente da função custo com relação aos parâmetros w.\n",
        "      dj_db (float): O gradiente da função custo com relação ao parâmetro b.\n",
        "    \"\"\"\n",
        "    n = len(x) \n",
        "    z = np.dot(x, w) + b\n",
        "    y_est = sigmoide(z) \n",
        "\n",
        "    erro = y_est - y\n",
        "\n",
        "    dj_dw = (1/n) * np.dot(x.T, erro)\n",
        "\n",
        "    dj_db = (1/n) * np.sum(erro)\n",
        "\n",
        "    return dj_db, dj_dw\n",
        "\n",
        "\n",
        "def metodo_do_gradiente(X, y, w_in, b_in, calcula_custo, calcula_gradiente, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Performa Método do Gradiente para aprender theta. Atualiza theta ao longo de\n",
        "    num_iters passos de iteração usando uma taxa de aprendizado alpha\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n))   : Dados, contendo m exemplos com n características\n",
        "      y (ndarray (m,))    : valores alvo\n",
        "      w_in (ndarray (n,)) : valores iniciais dos parâmetros w do modelo\n",
        "      b_in (escalar)      : valor inicial do parâmetro b do modelo\n",
        "      calcula_custo       : função que calcula o custo\n",
        "      calcula_gradiente   : função que calcula o gradiente\n",
        "      alpha (float)       : taxa de aprendizado\n",
        "      num_iters (int)     : Número de iterações para o método do gradiente\n",
        "\n",
        "    Retorna:\n",
        "      w (ndarray (n,)) : Valores atualizados para os parâmetros w\n",
        "      b (scalar)       : Valores atualizado para o parâmetro b\n",
        "      \"\"\"\n",
        "\n",
        "    # Valores históricos\n",
        "    J_history = []\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calcula o gradiente\n",
        "        dj_db,dj_dw = calcula_gradiente(X, y, w, b)\n",
        "\n",
        "        # Atualiza os parâmetros\n",
        "        w = w - alpha * dj_dw\n",
        "        b = b - alpha * dj_db\n",
        "\n",
        "        # Salva o custo\n",
        "        if i<100000:\n",
        "            J_history.append( calcula_custo(X, y, w, b))\n",
        "\n",
        "        # Faz print de tempos em tempos\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteração {i:4d}: Custo {J_history[-1]:8.2f}   \")\n",
        "\n",
        "    return w, b, J_history # retorna valores finais e históricos\n",
        "\n",
        "\n",
        "def rmsprop(X, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente, beta=0.9, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "        Aplica o método RMSProp para ajustar w, b para problemas multivariados.\n",
        "\n",
        "        Argumentos da função:\n",
        "            X (ndarray (m, n))  : Conjunto de dados com m exemplos e n características\n",
        "            y (ndarray (m,))    : Valores alvo de saída\n",
        "            w_in (ndarray (n,)) : Valores iniciais para os parâmetros w (vetor)\n",
        "            b_in (escalar)      : Valor inicial para o parâmetro b\n",
        "            alpha (float)       : Taxa de aprendizado\n",
        "            num_iters (int)     : Número de iterações\n",
        "            calcula_custo       : Função para calcular o custo\n",
        "            calcula_gradiente   : Função para calcular o gradiente\n",
        "            beta (float)        : Fator de decaimento para RMSProp (geralmente próximo de 0.9)\n",
        "            epsilon (float)     : Pequeno valor para evitar divisão por zero no cálculo do gradiente (geralmente 1e-8)\n",
        "\n",
        "        Retorna:\n",
        "            w (ndarray (n,))    : Valor atualizado de w\n",
        "            b (scalar)          : Valor atualizado de b\n",
        "            J_history (list)    : Histórico dos valores de custo\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializar variáveis\n",
        "    J_history = []\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "    \n",
        "    # Inicialização de acumuladores de gradiente como vetores do tamanho de w\n",
        "    v_dw = 0.0  # Vetor do mesmo tamanho que w\n",
        "    v_db = 0.0  # b é escalar\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Calcula o gradiente\n",
        "        dj_db, dj_dw = calcula_gradiente(X, y, w, b)\n",
        "\n",
        "        # Atualiza os acumuladores RMSProp para w e b (element-wise para w)\n",
        "        v_dw = beta * v_dw + (1 - beta) * (dj_dw**2)\n",
        "        v_db = beta * v_db + (1 - beta) * (dj_db**2)\n",
        "\n",
        "        # Atualiza os parâmetros w e b\n",
        "        w = w - alpha * dj_dw / (np.sqrt(v_dw) + epsilon)\n",
        "        b = b - alpha * dj_db / (np.sqrt(v_db) + epsilon)\n",
        "\n",
        "        # Armazena o custo para cada iteração\n",
        "        J_history.append(calcula_custo(X, y, w, b))\n",
        "        \n",
        "        # Exibe o progresso em intervalos regulares\n",
        "        if i % (num_iters // 10) == 0 or i == num_iters - 1:\n",
        "            print(f\"Iteração {i:4d}: Custo {J_history[-1]:8.2f}\")\n",
        "\n",
        "    return w, b, J_history\n",
        "\n",
        "\n",
        "def sgd(X, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente):\n",
        "    \"\"\"\n",
        "        Aplica o Método do Gradiente Estocástico (SGD) para ajustar w, b.\n",
        "\n",
        "        Argumentos da função:\n",
        "            X (ndarray (m, n))   : Conjunto de dados com m exemplos e n características\n",
        "            y (ndarray (m,))     : Valores alvo de saída\n",
        "            w_in (ndarray (n,))  : Valores iniciais para os parâmetros w (vetor)\n",
        "            b_in (scalar)        : Valor inicial para o parâmetro b\n",
        "            alpha (float)        : Taxa de aprendizado\n",
        "            num_iters (int)      : Número de iterações\n",
        "            calcula_custo        : Função para calcular o custo\n",
        "            calcula_gradiente    : Função para calcular o gradiente\n",
        "\n",
        "        Retorna:\n",
        "            w (ndarray (n,))     : Valor atualizado de w\n",
        "            b (scalar)           : Valor atualizado de b\n",
        "            J_history (list)     : Histórico dos valores de custo\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializar variáveis\n",
        "    J_history = []\n",
        "    b = b_in\n",
        "    w = w_in\n",
        "    m = len(y)  # Número de amostras\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Embaralhar os dados a cada iteração\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "\n",
        "        # Atualiza w e b para cada exemplo de treinamento\n",
        "        for j in range(m):\n",
        "            # Seleciona uma única amostra (vetor de características e valor alvo)\n",
        "            x_i = X_shuffled[j]   # Vetor (n,)\n",
        "            y_i = y_shuffled[j]   # Escalar\n",
        "            # print(np.array([x_i]), np.array([y_i]))\n",
        "            # break\n",
        "            # Calcula o gradiente para essa amostra\n",
        "            dj_db, dj_dw = calcula_gradiente(np.array([x_i]), np.array([y_i]), w, b)\n",
        "            # Atualiza os parâmetros w e b\n",
        "            w = w - alpha * dj_dw\n",
        "            b = b - alpha * dj_db\n",
        "\n",
        "        # Armazena o custo após a passagem por todo o conjunto de dados\n",
        "        J_history.append(calcula_custo(X, y, w, b))\n",
        "\n",
        "        # Exibe o progresso em intervalos regulares\n",
        "        if i % (num_iters // 10) == 0 or i == num_iters - 1:\n",
        "            print(f\"Iteração {i:4d}: Custo {J_history[-1]:8.2f}\")\n",
        "\n",
        "    return w, b, J_history\n",
        "\n",
        "\n",
        "def gradiente_conjugado(x, y, w_in, b_in, alpha, num_iters, calcula_custo, calcula_gradiente, tol=1e-8):\n",
        "    \"\"\"\n",
        "        Método de Gradiente Conjugado para ajuste de w e b.\n",
        "\n",
        "        Argumentos:\n",
        "            x (ndarray (m, n))  : Conjunto de dados com m amostras e n características\n",
        "            y (ndarray (m,))    : Valores alvo de saída\n",
        "            w_in (ndarray (n,)) : Vetor inicial para os parâmetros w\n",
        "            b_in (scalar)       : Valor inicial para o parâmetro b\n",
        "            alpha (float)       : Tamanho de passo ou taxa de aprendizado\n",
        "            num_iters (int)     : Número de iterações\n",
        "            calcula_custo       : Função para calcular o custo\n",
        "            calcula_gradiente   : Função para calcular o gradiente\n",
        "            tol (float)         : Tolerância para critério de parada\n",
        "\n",
        "        Retorna:\n",
        "            w (ndarray (n,))    : Valor atualizado de w\n",
        "            b (scalar)          : Valor atualizado de b\n",
        "            J_history (list)    : Histórico dos valores de custo\n",
        "    \"\"\"\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "    J_history = []\n",
        "\n",
        "    g_prev_w = np.zeros(len(w))\n",
        "    g_prev_b = 0\n",
        "    p_w = -g_prev_w  # Inicializa como negativo do gradiente\n",
        "    p_b = -g_prev_b\n",
        "\n",
        "    for t in range(1, num_iters + 1):\n",
        "        # Calcula o gradiente para w e b\n",
        "        dj_db, dj_dw = calcula_gradiente(x, y, w, b)\n",
        "\n",
        "        # Critério de parada baseado na norma do gradiente\n",
        "        if np.sqrt(np.sum(dj_dw ** 2)) < tol and abs(dj_db) < tol:\n",
        "            print(f\"Convergência atingida na iteração {t}.\")\n",
        "            break\n",
        "\n",
        "        # Primeira iteração - p inicial é o negativo do gradiente\n",
        "        if t == 1:\n",
        "            p_w = -dj_dw\n",
        "            p_b = -dj_db\n",
        "        else:\n",
        "            # Computa beta_t usando o método Polak-Ribiére\n",
        "            if np.dot(g_prev_w, g_prev_w) != 0:  # Prevenir divisão por zero\n",
        "                beta_t = np.dot(dj_dw, dj_dw - g_prev_w) / np.dot(g_prev_w, g_prev_w)\n",
        "            else:\n",
        "                beta_t = 0  # Ou outra abordagem\n",
        "            \n",
        "            p_w = -dj_dw + beta_t * p_w\n",
        "            p_b = -dj_db + beta_t * p_b\n",
        "\n",
        "        # Atualiza os parâmetros\n",
        "        w += alpha * p_w\n",
        "        b += alpha * p_b\n",
        "\n",
        "        # Calcula o custo e armazena no histórico\n",
        "        J_history.append(calcula_custo(x, y, w, b))\n",
        "\n",
        "        # Atualiza o gradiente anterior\n",
        "        g_prev_w = dj_dw\n",
        "        g_prev_b = dj_db\n",
        "\n",
        "        # Exibe o progresso em intervalos regulares\n",
        "        if t % (num_iters // 10) == 0 or t == num_iters - 1:\n",
        "            print(f\"Iteração {t:4d}: Custo {J_history[-1]:8.2f}\")\n",
        "\n",
        "    return w, b, J_history\n",
        "\n",
        "\n",
        "def adam(x, y, w_in, b_in, alpha, beta1, beta2, epsilon, num_iters, calcula_custo, calcula_gradiente):\n",
        "    \"\"\"\n",
        "        Aplica o algoritmo Adam para otimização de w e b em um cenário multivariado.\n",
        "\n",
        "        Argumentos:\n",
        "            x (ndarray): Conjunto de dados com m amostras e n características.\n",
        "            y (ndarray): Valores alvo de saída.\n",
        "            w_in (ndarray): Valores iniciais para os parâmetros w (vetor de pesos).\n",
        "            b_in (scalar): Valor inicial para o parâmetro b (intercepto).\n",
        "            alpha (float): Taxa de aprendizado.\n",
        "            beta1 (float): Exponencial para a média dos gradientes.\n",
        "            beta2 (float): Exponencial para a média dos gradientes ao quadrado.\n",
        "            epsilon (float): Pequeno valor para evitar divisão por zero.\n",
        "            num_iters (int): Número total de iterações.\n",
        "            calcula_custo: Função que calcula o custo.\n",
        "            calcula_gradiente: Função que calcula o gradiente.\n",
        "\n",
        "        Retorna:\n",
        "            w (ndarray): Valor otimizado de w (vetor de pesos).\n",
        "            b (scalar): Valor otimizado de b (intercepto).\n",
        "            J_history (list): Histórico dos valores de custo.\n",
        "    \"\"\"\n",
        "    w = w_in\n",
        "    b = b_in\n",
        "    m_w, v_w = 0, 0  # Inicialização dos momentos de w\n",
        "    m_b, v_b = 0, 0  # Inicialização dos momentos de b\n",
        "\n",
        "    J_history = []\n",
        "\n",
        "    for t in range(1, num_iters + 1):\n",
        "        # Calcula o gradiente\n",
        "        dj_db, dj_dw = calcula_gradiente(x, y, w, b)\n",
        "\n",
        "        # Atualiza as médias e variâncias dos gradientes (momentos)\n",
        "        m_w = beta1 * m_w + (1 - beta1) * dj_dw\n",
        "        m_b = beta1 * m_b + (1 - beta1) * dj_db\n",
        "\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (dj_dw ** 2)\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (dj_db ** 2)\n",
        "\n",
        "        # Correções de bias\n",
        "        m_w_corrigido = m_w / (1 - beta1 ** t)\n",
        "        m_b_corrigido = m_b / (1 - beta1 ** t)\n",
        "        v_w_corrigido = v_w / (1 - beta2 ** t)\n",
        "        v_b_corrigido = v_b / (1 - beta2 ** t)\n",
        "\n",
        "        # Atualiza os parâmetros\n",
        "        w -= alpha * (m_w_corrigido / (np.sqrt(v_w_corrigido) + epsilon))\n",
        "        b -= alpha * (m_b_corrigido / (np.sqrt(v_b_corrigido) + epsilon))\n",
        "\n",
        "        # Armazena o custo atual\n",
        "        J_history.append(calcula_custo(x, y, w, b))\n",
        "        \n",
        "        # Exibe o progresso em intervalos regulares\n",
        "        if t % (num_iters // 10) == 0 or t == num_iters - 1:\n",
        "            print(f\"Iteração {t:4d}: Custo {J_history[-1]:8.2f}   \")\n",
        "\n",
        "    return w, b, J_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518f3a28",
      "metadata": {},
      "source": [
        "##### Otimização - Método do Gradiente (Vamos verificar se nada de braçada mesmo....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "id": "e5c5c8e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteração    0: Custo     0.69   \n",
            "Iteração   50: Custo     0.15   \n",
            "Iteração  100: Custo     0.08   \n",
            "Iteração  150: Custo     0.06   \n",
            "Iteração  200: Custo     0.04   \n",
            "Iteração  250: Custo     0.03   \n",
            "Iteração  300: Custo     0.03   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteração  350: Custo     0.02   \n",
            "Iteração  400: Custo     0.02   \n",
            "Iteração  450: Custo     0.02   \n",
            "b,w encontrados pelo método do gradiente: -14.24,[5.28920767 5.08615032] \n",
            "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
            "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
            "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
            "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
            "| $y_{est}^{\\left(i\\right)}$ | 0.0185       | 0.0205       | 0.0226       | 0.9847       | 0.9985       | 0.9772       |\n",
            "| ${Perda}^{\\left(i\\right)}$ | 10.2739      | 10.3754      | 10.4769      | 0.0000       | 0.0000       | 0.0000       |\n",
            "Custo: 0.0170\n"
          ]
        }
      ],
      "source": [
        "# inicializando parâmetros\n",
        "m,n       = X.shape\n",
        "w_inicial = np.zeros((n,))\n",
        "b_inicial = 0\n",
        "# parâmetros para rodar o Método do Gradiente:\n",
        "num_iters = 500\n",
        "alpha      = 2 # Otimizado usando VC (Vai Cavalo!!!)\n",
        "# Rodando o Método do gradiente\n",
        "\n",
        "w_final, b_final, J_hist = metodo_do_gradiente(X, y, w_inicial, b_inicial,\n",
        "                                                    calcula_custo, calcula_gradiente,\n",
        "                                                    alpha, num_iters)\n",
        "print(f\"b,w encontrados pelo método do gradiente: {b_final:0.2f},{w_final} \")\n",
        "\n",
        "\n",
        "z = np.dot(X, w_final) + b_final\n",
        "# calculando a função sigmóide para cada ponto\n",
        "y_pred = sigmoide(z)\n",
        "\n",
        "perdas = calcula_perda(X, y, w_final, b_inicial)\n",
        "\n",
        "custo = calcula_custo(X, y, w_final, b_final)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\")\n",
        "xc = [\"({:.2f}, {:.2f})\".format(x[0], x[1]) for x in X]\n",
        "yc = [\"{:.4f}\".format(x) for x in y]\n",
        "yc.insert(0, \"$y^{\\left(i\\\\right)}$\") \n",
        "xc.insert(0,\"$X^{\\left(i\\\\right)}$\")\n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"$y_{est}^{\\left(i\\\\right)}$\")\n",
        "perdas = [\"{:.4f}\".format(x) for x in perdas]\n",
        "perdas.insert(0, \"${Perda}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(xc)\n",
        "tabela.append(yc)\n",
        "tabela.append(y_predc)\n",
        "tabela.append(perdas)\n",
        "\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))\n",
        "\n",
        "print(f\"Custo: {custo:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d7e6fd",
      "metadata": {},
      "source": [
        "##### Otimização - RMSProp (Vamos verificar se nada de braçada mesmo....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "id": "43cff8dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteração    0: Custo     6.32\n",
            "Iteração   10: Custo     0.09\n",
            "Iteração   20: Custo     3.11\n",
            "Iteração   30: Custo     0.03\n",
            "Iteração   40: Custo     0.02\n",
            "Iteração   50: Custo     0.02\n",
            "Iteração   60: Custo     0.01\n",
            "Iteração   70: Custo     0.01\n",
            "Iteração   80: Custo     0.01\n",
            "Iteração   90: Custo     0.00\n",
            "Iteração   99: Custo     0.00\n",
            "b,w encontrados pelo método do RMSProp: -22.08,[8.11392118 7.92387849] \n",
            "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
            "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
            "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
            "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
            "| $y_{est}^{\\left(i\\right)}$ | 0.0022       | 0.0024       | 0.0026       | 0.9980       | 1.0000       | 0.9971       |\n",
            "| ${Perda}^{\\left(i\\right)}$ | 15.9428      | 16.0378      | 16.1328      | 0.0000       | 0.0000       | 0.0000       |\n",
            "Custo: 0.0020\n"
          ]
        }
      ],
      "source": [
        "# inicializando parâmetros\n",
        "m,n       = X.shape\n",
        "w_inicial = np.zeros((n,))\n",
        "b_inicial = 0\n",
        "# parâmetros para rodar o Método do Gradiente:\n",
        "num_iters = 100\n",
        "alpha      = 2 # Otimizado usando VC (Vai Cavalo!!!)\n",
        "# Rodando o Método do gradiente\n",
        "\n",
        "w_final, b_final, J_hist = rmsprop(X, y, w_inicial, b_inicial, alpha, num_iters,\n",
        "                                                    calcula_custo, calcula_gradiente)\n",
        "\n",
        "print(f\"b,w encontrados pelo método do RMSProp: {b_final:0.2f},{w_final} \")\n",
        "\n",
        "\n",
        "z = np.dot(X, w_final) + b_final\n",
        "# calculando a função sigmóide para cada ponto\n",
        "y_pred = sigmoide(z)\n",
        "\n",
        "perdas = calcula_perda(X, y, w_final, b_inicial)\n",
        "\n",
        "custo = calcula_custo(X, y, w_final, b_final)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\")\n",
        "xc = [\"({:.2f}, {:.2f})\".format(x[0], x[1]) for x in X]\n",
        "yc = [\"{:.4f}\".format(x) for x in y]\n",
        "yc.insert(0, \"$y^{\\left(i\\\\right)}$\") \n",
        "xc.insert(0, \"$X^{\\left(i\\\\right)}$\")\n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"$y_{est}^{\\left(i\\\\right)}$\")\n",
        "perdas = [\"{:.4f}\".format(x) for x in perdas]\n",
        "perdas.insert(0, \"${Perda}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(xc)\n",
        "tabela.append(yc)\n",
        "tabela.append(y_predc)\n",
        "tabela.append(perdas)\n",
        "\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))\n",
        "\n",
        "print(f\"Custo: {custo:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e041a9ed",
      "metadata": {},
      "source": [
        "##### Otimização - SGD (Vamos verificar se nada de braçada mesmo....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "id": "4fd807fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteração    0: Custo     0.76\n",
            "Iteração  100: Custo     0.05\n",
            "Iteração  200: Custo     0.03\n",
            "Iteração  300: Custo     0.02\n",
            "Iteração  400: Custo     0.01\n",
            "Iteração  500: Custo     0.01\n",
            "Iteração  600: Custo     0.01\n",
            "Iteração  700: Custo     0.01\n",
            "Iteração  800: Custo     0.01\n",
            "Iteração  900: Custo     0.01\n",
            "Iteração  999: Custo     0.01\n",
            "b,w encontrados pelo método SGD: -18.39,[6.79350193 6.59353241] \n",
            "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
            "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
            "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
            "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
            "| $y_{est}^{\\left(i\\right)}$ | 0.0060       | 0.0067       | 0.0074       | 0.9950       | 0.9998       | 0.9925       |\n",
            "| ${Perda}^{\\left(i\\right)}$ | 13.2871      | 13.3870      | 13.4870      | 0.0000       | 0.0000       | 0.0000       |\n",
            "Custo: 0.0055\n"
          ]
        }
      ],
      "source": [
        "# inicializando parâmetros\n",
        "m,n       = X.shape\n",
        "w_inicial = np.zeros((n,))\n",
        "b_inicial = 0\n",
        "# parâmetros para rodar o Método do Gradiente:\n",
        "num_iters = 1000\n",
        "alpha      = 0.5 # Otimizado usando VC (Vai Cavalo!!!)\n",
        "\n",
        "# Rodando o Método do gradiente\n",
        "w_final, b_final, J_hist = sgd(X, y, w_inicial, b_inicial, alpha, num_iters,\n",
        "                                                    calcula_custo, calcula_gradiente)\n",
        "\n",
        "print(f\"b,w encontrados pelo método SGD: {b_final:0.2f},{w_final} \")\n",
        "\n",
        "\n",
        "\n",
        "z = np.dot(X, w_final) + b_final\n",
        "# calculando a função sigmóide para cada ponto\n",
        "y_pred = sigmoide(z)\n",
        "\n",
        "perdas = calcula_perda(X, y, w_final, b_inicial)\n",
        "\n",
        "custo = calcula_custo(X, y, w_final, b_final)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\")\n",
        "xc = [\"({:.2f}, {:.2f})\".format(x[0], x[1]) for x in X]\n",
        "yc = [\"{:.4f}\".format(x) for x in y]\n",
        "yc.insert(0, \"$y^{\\left(i\\\\right)}$\") \n",
        "xc.insert(0, \"$X^{\\left(i\\\\right)}$\")\n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"$y_{est}^{\\left(i\\\\right)}$\")\n",
        "perdas = [\"{:.4f}\".format(x) for x in perdas]\n",
        "perdas.insert(0, \"${Perda}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(xc)\n",
        "tabela.append(yc)\n",
        "tabela.append(y_predc)\n",
        "tabela.append(perdas)\n",
        "\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))\n",
        "\n",
        "print(f\"Custo: {custo:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d3d4fc",
      "metadata": {},
      "source": [
        "##### Otimização - Gradiente Conjugado (Vamos verificar se nada de braçada mesmo....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "id": "e345a399",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteração   20: Custo     0.07\n",
            "Iteração   40: Custo     0.01\n",
            "Iteração   60: Custo     0.01\n",
            "Iteração   80: Custo     0.01\n",
            "Iteração  100: Custo     0.00\n",
            "Iteração  120: Custo     0.00\n",
            "Iteração  140: Custo     0.00\n",
            "Iteração  160: Custo     0.00\n",
            "Iteração  180: Custo     0.00\n",
            "Iteração  199: Custo     0.00\n",
            "Iteração  200: Custo     0.00\n",
            "b,w encontrados pelo método do Gradiente Conjugado: -20.57,[7.7605974  7.21130688] \n",
            "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
            "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
            "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
            "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
            "| $y_{est}^{\\left(i\\right)}$ | 0.0028       | 0.0037       | 0.0049       | 0.9982       | 0.9999       | 0.9946       |\n",
            "| ${Perda}^{\\left(i\\right)}$ | 14.6973      | 14.9719      | 15.2465      | 0.0000       | 0.0000       | 0.0000       |\n",
            "Custo: 0.0031\n"
          ]
        }
      ],
      "source": [
        "# inicializando parâmetros\n",
        "m,n       = X.shape\n",
        "w_inicial = np.zeros((n,))\n",
        "b_inicial = 0\n",
        "# parâmetros para rodar o Método do Gradiente:\n",
        "num_iters = 200\n",
        "alpha      = 5 # Otimizado usando VC (Vai Cavalo!!!)\n",
        "\n",
        "w_final, b_final, J_hist = gradiente_conjugado(X, y, w_inicial, b_inicial, alpha, num_iters,\n",
        "                                                    calcula_custo, calcula_gradiente)\n",
        "\n",
        "print(f\"b,w encontrados pelo método do Gradiente Conjugado: {b_final:0.2f},{w_final} \")\n",
        "\n",
        "\n",
        "\n",
        "z = np.dot(X, w_final) + b_final\n",
        "# calculando a função sigmóide para cada ponto\n",
        "y_pred = sigmoide(z)\n",
        "\n",
        "perdas = calcula_perda(X, y, w_final, b_inicial)\n",
        "\n",
        "custo = calcula_custo(X, y, w_final, b_final)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\")\n",
        "xc = [\"({:.2f}, {:.2f})\".format(x[0], x[1]) for x in X]\n",
        "yc = [\"{:.4f}\".format(x) for x in y]\n",
        "yc.insert(0, \"$y^{\\left(i\\\\right)}$\") \n",
        "xc.insert(0, \"$X^{\\left(i\\\\right)}$\")\n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"$y_{est}^{\\left(i\\\\right)}$\")\n",
        "perdas = [\"{:.4f}\".format(x) for x in perdas]\n",
        "perdas.insert(0, \"${Perda}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(xc)\n",
        "tabela.append(yc)\n",
        "tabela.append(y_predc)\n",
        "tabela.append(perdas)\n",
        "\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))\n",
        "\n",
        "print(f\"Custo: {custo:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "id": "f02dbba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Otimização - Adam (Vamos verificar se nada de braçada mesmo....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "id": "0aa48b47",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteração   20: Custo     0.13   \n",
            "Iteração   40: Custo     0.05   \n",
            "Iteração   60: Custo     0.03   \n",
            "Iteração   80: Custo     0.02   \n",
            "Iteração  100: Custo     0.01   \n",
            "Iteração  120: Custo     0.01   \n",
            "Iteração  140: Custo     0.01   \n",
            "Iteração  160: Custo     0.01   \n",
            "Iteração  180: Custo     0.01   \n",
            "Iteração  199: Custo     0.01   \n",
            "Iteração  200: Custo     0.01   \n",
            "b,w encontrados pelo método Adam: -18.01,[6.64355398 6.44880118] \n",
            "| $i$                        | $1$          | $2$          | $3$          | $4$          | $5$          | $6$          |\n",
            "|:---------------------------|:-------------|:-------------|:-------------|:-------------|:-------------|:-------------|\n",
            "| $X^{\\left(i\\right)}$       | (0.50, 1.50) | (1.00, 1.00) | (1.50, 0.50) | (3.00, 0.50) | (2.00, 2.00) | (1.00, 2.50) |\n",
            "| $y^{\\left(i\\right)}$       | 0.0000       | 0.0000       | 0.0000       | 1.0000       | 1.0000       | 1.0000       |\n",
            "| $y_{est}^{\\left(i\\right)}$ | 0.0066       | 0.0073       | 0.0080       | 0.9942       | 0.9997       | 0.9915       |\n",
            "| ${Perda}^{\\left(i\\right)}$ | 12.9950      | 13.0924      | 13.1897      | 0.0000       | 0.0000       | 0.0000       |\n",
            "Custo: 0.0061\n"
          ]
        }
      ],
      "source": [
        "# inicializando parâmetros\n",
        "m,n       = X.shape\n",
        "w_inicial = np.zeros((n,))\n",
        "b_inicial = 0\n",
        "# parâmetros para rodar o Método do Gradiente:\n",
        "num_iters = 200\n",
        "alpha  = 1 # otimizado usando VC (Vai Cavalo)\n",
        "beta1 = 0.9 # otimizado usando VC (Vai Cavalo)\n",
        "beta2 = 0.999 # otimizado usando VC (Vai Cavalo)\n",
        "epsilon = 1e-8 # otimizado usando VC (Vai Cavalo)\n",
        "# Rodando o Método do gradiente\n",
        "w_final, b_final, J_hist = adam(X, y, w_inicial, b_inicial, alpha, beta1, beta2, epsilon, num_iters,\n",
        "                                                    calcula_custo, calcula_gradiente)\n",
        "\n",
        "print(f\"b,w encontrados pelo método Adam: {b_final:0.2f},{w_final} \")\n",
        "\n",
        "\n",
        "z = np.dot(X, w_final) + b_final\n",
        "# calculando a função sigmóide para cada ponto\n",
        "y_pred = sigmoide(z)\n",
        "\n",
        "perdas = calcula_perda(X, y, w_final, b_inicial)\n",
        "\n",
        "custo = calcula_custo(X, y, w_final, b_final)\n",
        "\n",
        "# construindo a tabelinha - vamos colocá-la na resposta da pergunta\n",
        "\n",
        "tabela = list()\n",
        "header = [\"${:d}$\".format(x+1) for x in range(len(X))]\n",
        "header.insert(0, \"$i$\")\n",
        "xc = [\"({:.2f}, {:.2f})\".format(x[0], x[1]) for x in X]\n",
        "yc = [\"{:.4f}\".format(x) for x in y]\n",
        "yc.insert(0, \"$y^{\\left(i\\\\right)}$\") \n",
        "xc.insert(0, \"$X^{\\left(i\\\\right)}$\")\n",
        "y_predc = [\"{:.4f}\".format(x) for x in y_pred]\n",
        "y_predc.insert(0, \"$y_{est}^{\\left(i\\\\right)}$\")\n",
        "perdas = [\"{:.4f}\".format(x) for x in perdas]\n",
        "perdas.insert(0, \"${Perda}^{\\left(i\\\\right)}$\")\n",
        "tabela.append(xc)\n",
        "tabela.append(yc)\n",
        "tabela.append(y_predc)\n",
        "tabela.append(perdas)\n",
        "\n",
        "print(tabulate(tabela,\n",
        "               headers=header,\n",
        "               tablefmt='pipe'))\n",
        "\n",
        "print(f\"Custo: {custo:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
